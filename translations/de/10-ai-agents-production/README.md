<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cdfd0acc8592c1af14f8637833450375",
  "translation_date": "2025-08-30T07:21:47+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "de"
}
-->
# KI-Agenten in der Produktion: Beobachtbarkeit & Bewertung

[![KI-Agenten in der Produktion](../../../translated_images/lesson-10-thumbnail.2b79a30773db093e0b4fb47aaa618069e0afb4745fad4836526cf51df87f9ac9.de.png)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

Wenn KI-Agenten von experimentellen Prototypen zu realen Anwendungen √ºbergehen, wird die F√§higkeit, ihr Verhalten zu verstehen, ihre Leistung zu √ºberwachen und ihre Ergebnisse systematisch zu bewerten, entscheidend.

## Lernziele

Nach Abschluss dieser Lektion werden Sie wissen/verstehen:
- Grundkonzepte der Beobachtbarkeit und Bewertung von Agenten
- Techniken zur Verbesserung der Leistung, Kosten und Effektivit√§t von Agenten
- Was und wie Sie Ihre KI-Agenten systematisch bewerten k√∂nnen
- Wie Sie Kosten bei der Bereitstellung von KI-Agenten in der Produktion kontrollieren
- Wie Sie Agenten, die mit AutoGen erstellt wurden, instrumentieren

Das Ziel ist, Sie mit dem Wissen auszustatten, um Ihre "Black-Box"-Agenten in transparente, handhabbare und zuverl√§ssige Systeme zu verwandeln.

_**Hinweis:** Es ist wichtig, KI-Agenten bereitzustellen, die sicher und vertrauensw√ºrdig sind. Schauen Sie sich dazu die Lektion [Vertrauensw√ºrdige KI-Agenten erstellen](./06-building-trustworthy-agents/README.md) an._

## Traces und Spans

Beobachtungswerkzeuge wie [Langfuse](https://langfuse.com/) oder [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) stellen Agentenl√§ufe in der Regel als Traces und Spans dar.

- **Trace** repr√§sentiert eine vollst√§ndige Agentenaufgabe von Anfang bis Ende (z. B. die Bearbeitung einer Benutzeranfrage).
- **Spans** sind einzelne Schritte innerhalb des Traces (z. B. der Aufruf eines Sprachmodells oder das Abrufen von Daten).

![Trace-Baum in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Ohne Beobachtbarkeit kann sich ein KI-Agent wie eine "Black Box" anf√ºhlen ‚Äì sein interner Zustand und seine Logik sind undurchsichtig, was es schwierig macht, Probleme zu diagnostizieren oder die Leistung zu optimieren. Mit Beobachtbarkeit werden Agenten zu "Glasboxen", die Transparenz bieten, was entscheidend ist, um Vertrauen aufzubauen und sicherzustellen, dass sie wie beabsichtigt funktionieren.

## Warum Beobachtbarkeit in Produktionsumgebungen wichtig ist

Der √úbergang von KI-Agenten in Produktionsumgebungen bringt neue Herausforderungen und Anforderungen mit sich. Beobachtbarkeit ist nicht mehr nur ein "Nice-to-have", sondern eine kritische F√§higkeit:

*   **Fehlerbehebung und Ursachenanalyse**: Wenn ein Agent fehlschl√§gt oder unerwartete Ergebnisse liefert, bieten Beobachtungswerkzeuge die Traces, die ben√∂tigt werden, um die Fehlerquelle zu identifizieren. Dies ist besonders wichtig bei komplexen Agenten, die mehrere LLM-Aufrufe, Tool-Interaktionen und bedingte Logik umfassen k√∂nnen.
*   **Latenz- und Kostenmanagement**: KI-Agenten verlassen sich oft auf LLMs und andere externe APIs, die pro Token oder pro Aufruf abgerechnet werden. Beobachtbarkeit erm√∂glicht eine pr√§zise Nachverfolgung dieser Aufrufe, um Operationen zu identifizieren, die √ºberm√§√üig langsam oder teuer sind. So k√∂nnen Teams Prompts optimieren, effizientere Modelle ausw√§hlen oder Workflows neu gestalten, um Betriebskosten zu senken und eine gute Benutzererfahrung sicherzustellen.
*   **Vertrauen, Sicherheit und Compliance**: In vielen Anwendungen ist es wichtig, dass Agenten sicher und ethisch handeln. Beobachtbarkeit bietet eine Pr√ºfspur der Aktionen und Entscheidungen des Agenten. Dies kann genutzt werden, um Probleme wie Prompt-Injection, die Generierung sch√§dlicher Inhalte oder den unsachgem√§√üen Umgang mit personenbezogenen Daten (PII) zu erkennen und zu beheben. Beispielsweise k√∂nnen Sie Traces √ºberpr√ºfen, um zu verstehen, warum ein Agent eine bestimmte Antwort gegeben oder ein bestimmtes Tool verwendet hat.
*   **Kontinuierliche Verbesserungszyklen**: Beobachtungsdaten bilden die Grundlage f√ºr einen iterativen Entwicklungsprozess. Durch die √úberwachung der Leistung von Agenten in der realen Welt k√∂nnen Teams Verbesserungsbereiche identifizieren, Daten f√ºr die Feinabstimmung von Modellen sammeln und die Auswirkungen von √Ñnderungen validieren. Dies schafft einen Feedback-Zyklus, bei dem Produktionserkenntnisse aus der Online-Bewertung die Offline-Experimente und -Verfeinerungen informieren, was zu einer schrittweisen Verbesserung der Agentenleistung f√ºhrt.

## Wichtige Metriken zur √úberwachung

Um das Verhalten von Agenten zu √ºberwachen und zu verstehen, sollten eine Reihe von Metriken und Signalen verfolgt werden. W√§hrend die spezifischen Metriken je nach Zweck des Agenten variieren k√∂nnen, sind einige universell wichtig.

Hier sind einige der h√§ufigsten Metriken, die von Beobachtungswerkzeugen √ºberwacht werden:

**Latenz:** Wie schnell reagiert der Agent? Lange Wartezeiten beeintr√§chtigen die Benutzererfahrung negativ. Sie sollten die Latenz f√ºr Aufgaben und einzelne Schritte durch die Nachverfolgung von Agentenl√§ufen messen. Zum Beispiel k√∂nnte ein Agent, der 20 Sekunden f√ºr alle Modellaufrufe ben√∂tigt, durch die Verwendung eines schnelleren Modells oder durch paralleles Ausf√ºhren von Modellaufrufen beschleunigt werden.

**Kosten:** Wie hoch sind die Kosten pro Agentenlauf? KI-Agenten verlassen sich auf LLM-Aufrufe, die pro Token oder externe APIs abgerechnet werden. H√§ufige Tool-Nutzung oder mehrere Prompts k√∂nnen die Kosten schnell erh√∂hen. Wenn ein Agent beispielsweise ein LLM f√ºnfmal aufruft, um eine marginale Qualit√§tsverbesserung zu erzielen, m√ºssen Sie bewerten, ob die Kosten gerechtfertigt sind oder ob Sie die Anzahl der Aufrufe reduzieren oder ein g√ºnstigeres Modell verwenden k√∂nnen. Echtzeit√ºberwachung kann auch unerwartete Spitzen identifizieren (z. B. Fehler, die √ºberm√§√üige API-Schleifen verursachen).

**Anforderungsfehler:** Wie viele Anfragen hat der Agent nicht erf√ºllt? Dies kann API-Fehler oder fehlgeschlagene Tool-Aufrufe umfassen. Um Ihren Agenten in der Produktion robuster gegen diese Fehler zu machen, k√∂nnen Sie Fallbacks oder Wiederholungen einrichten. Zum Beispiel, wenn Anbieter A f√ºr LLMs ausf√§llt, wechseln Sie zu Anbieter B als Backup.

**Benutzerfeedback:** Direkte Benutzerbewertungen liefern wertvolle Einblicke. Dies kann explizite Bewertungen (üëçDaumen hoch/üëérunter, ‚≠ê1-5 Sterne) oder Textkommentare umfassen. Konsistentes negatives Feedback sollte Sie alarmieren, da dies ein Zeichen daf√ºr ist, dass der Agent nicht wie erwartet funktioniert.

**Implizites Benutzerfeedback:** Benutzerverhalten liefert indirektes Feedback, auch ohne explizite Bewertungen. Dies kann das sofortige Umformulieren von Fragen, wiederholte Anfragen oder das Klicken auf eine Wiederholungsschaltfl√§che umfassen. Wenn Sie beispielsweise sehen, dass Benutzer wiederholt dieselbe Frage stellen, ist dies ein Zeichen daf√ºr, dass der Agent nicht wie erwartet funktioniert.

**Genauigkeit:** Wie oft liefert der Agent korrekte oder w√ºnschenswerte Ergebnisse? Die Definition von Genauigkeit variiert (z. B. Probleml√∂sungsgenauigkeit, Genauigkeit bei der Informationsbeschaffung, Benutzerzufriedenheit). Der erste Schritt besteht darin, zu definieren, wie Erfolg f√ºr Ihren Agenten aussieht. Sie k√∂nnen die Genauigkeit durch automatisierte Pr√ºfungen, Bewertungsergebnisse oder Task-Completion-Labels verfolgen. Zum Beispiel k√∂nnen Traces als "erfolgreich" oder "fehlgeschlagen" markiert werden.

**Automatisierte Bewertungsmetriken:** Sie k√∂nnen auch automatisierte Bewertungen einrichten. Beispielsweise k√∂nnen Sie ein LLM verwenden, um die Ausgabe des Agenten zu bewerten, z. B. ob sie hilfreich, genau oder nicht ist. Es gibt auch mehrere Open-Source-Bibliotheken, die Ihnen helfen, verschiedene Aspekte des Agenten zu bewerten, z. B. [RAGAS](https://docs.ragas.io/) f√ºr RAG-Agenten oder [LLM Guard](https://llm-guard.com/), um sch√§dliche Sprache oder Prompt-Injection zu erkennen.

In der Praxis bietet eine Kombination dieser Metriken die beste Abdeckung f√ºr die Gesundheit eines KI-Agenten. Im [Beispiel-Notebook](./code_samples/10_autogen_evaluation.ipynb) dieses Kapitels zeigen wir Ihnen, wie diese Metriken in realen Beispielen aussehen, aber zuerst lernen wir, wie ein typischer Bewertungsworkflow aussieht.

## Instrumentieren Ihres Agenten

Um Tracing-Daten zu sammeln, m√ºssen Sie Ihren Code instrumentieren. Ziel ist es, den Agenten-Code so zu instrumentieren, dass Traces und Metriken ausgegeben werden, die von einer Beobachtungsplattform erfasst, verarbeitet und visualisiert werden k√∂nnen.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) hat sich als Industriestandard f√ºr die Beobachtbarkeit von LLMs etabliert. Es bietet eine Reihe von APIs, SDKs und Tools zum Generieren, Sammeln und Exportieren von Telemetriedaten.

Es gibt viele Instrumentierungsbibliotheken, die bestehende Agenten-Frameworks umschlie√üen und das Exportieren von OpenTelemetry-Spans an ein Beobachtungswerkzeug erleichtern. Unten finden Sie ein Beispiel f√ºr die Instrumentierung eines AutoGen-Agenten mit der [OpenLit-Instrumentierungsbibliothek](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

Das [Beispiel-Notebook](./code_samples/10_autogen_evaluation.ipynb) in diesem Kapitel zeigt, wie Sie Ihren AutoGen-Agenten instrumentieren.

**Manuelle Span-Erstellung:** W√§hrend Instrumentierungsbibliotheken eine gute Grundlage bieten, gibt es oft F√§lle, in denen detailliertere oder benutzerdefinierte Informationen ben√∂tigt werden. Sie k√∂nnen Spans manuell erstellen, um benutzerdefinierte Anwendungslogik hinzuzuf√ºgen. Wichtiger ist, dass sie automatisch oder manuell erstellte Spans mit benutzerdefinierten Attributen (auch als Tags oder Metadaten bekannt) anreichern k√∂nnen. Diese Attribute k√∂nnen gesch√§ftsspezifische Daten, Zwischenberechnungen oder jeden Kontext umfassen, der f√ºr Debugging oder Analyse n√ºtzlich sein k√∂nnte, wie `user_id`, `session_id` oder `model_version`.

Beispiel f√ºr die manuelle Erstellung von Traces und Spans mit dem [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Agentenbewertung

Beobachtbarkeit liefert uns Metriken, aber die Bewertung ist der Prozess der Analyse dieser Daten (und Durchf√ºhrung von Tests), um festzustellen, wie gut ein KI-Agent funktioniert und wie er verbessert werden kann. Mit anderen Worten, sobald Sie diese Traces und Metriken haben, wie nutzen Sie sie, um den Agenten zu beurteilen und Entscheidungen zu treffen?

Regelm√§√üige Bewertung ist wichtig, da KI-Agenten oft nicht-deterministisch sind und sich entwickeln k√∂nnen (durch Updates oder driftendes Modellverhalten) ‚Äì ohne Bewertung w√ºssten Sie nicht, ob Ihr "intelligenter Agent" tats√§chlich seine Aufgabe gut erf√ºllt oder ob er sich verschlechtert hat.

Es gibt zwei Kategorien von Bewertungen f√ºr KI-Agenten: **Offline-Bewertung** und **Online-Bewertung**. Beide sind wertvoll und erg√§nzen sich gegenseitig. Normalerweise beginnen wir mit der Offline-Bewertung, da dies der Mindestschritt ist, bevor ein Agent bereitgestellt wird.

### Offline-Bewertung

![Datensatz-Elemente in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Dies beinhaltet die Bewertung des Agenten in einer kontrollierten Umgebung, typischerweise mit Testdatens√§tzen, nicht mit Live-Benutzeranfragen. Sie verwenden kuratierte Datens√§tze, bei denen Sie wissen, was die erwartete Ausgabe oder das korrekte Verhalten ist, und f√ºhren dann Ihren Agenten darauf aus.

Wenn Sie beispielsweise einen Agenten f√ºr mathematische Textaufgaben erstellt haben, k√∂nnten Sie einen [Testdatensatz](https://huggingface.co/datasets/gsm8k) mit 100 Aufgaben und bekannten Antworten haben. Die Offline-Bewertung wird oft w√§hrend der Entwicklung durchgef√ºhrt (und kann Teil von CI/CD-Pipelines sein), um Verbesserungen zu √ºberpr√ºfen oder Regressionen zu verhindern. Der Vorteil ist, dass sie **wiederholbar ist und Sie klare Genauigkeitsmetriken erhalten k√∂nnen, da Sie eine Ground Truth haben**. Sie k√∂nnten auch Benutzeranfragen simulieren und die Antworten des Agenten mit idealen Antworten vergleichen oder automatisierte Metriken wie oben beschrieben verwenden.

Die gr√∂√üte Herausforderung bei der Offline-Bewertung besteht darin, sicherzustellen, dass Ihr Testdatensatz umfassend und relevant bleibt ‚Äì der Agent k√∂nnte auf einem festen Testdatensatz gut abschneiden, aber auf sehr unterschiedliche Anfragen in der Produktion sto√üen. Daher sollten Sie Testdatens√§tze mit neuen Randf√§llen und Beispielen aktualisieren, die reale Szenarien widerspiegeln. Eine Mischung aus kleinen "Smoke-Test"-F√§llen und gr√∂√üeren Bewertungss√§tzen ist n√ºtzlich: kleine S√§tze f√ºr schnelle √úberpr√ºfungen und gr√∂√üere f√ºr umfassendere Leistungsmetriken.

### Online-Bewertung

![√úbersicht der Beobachtungsmetriken](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Dies bezieht sich auf die Bewertung des Agenten in einer Live-Umgebung, d. h. w√§hrend der tats√§chlichen Nutzung in der Produktion. Die Online-Bewertung umfasst die kontinuierliche √úberwachung der Leistung des Agenten bei realen Benutzerinteraktionen und die Analyse der Ergebnisse.

Beispielsweise k√∂nnten Sie Erfolgsraten, Benutzerzufriedenheitsbewertungen oder andere Metriken im Live-Traffic verfolgen. Der Vorteil der Online-Bewertung ist, dass sie **Dinge erfasst, die Sie in einer Laboreinstellung m√∂glicherweise nicht vorhersehen k√∂nnen** ‚Äì Sie k√∂nnen Modell-Drift im Laufe der Zeit beobachten (wenn die Effektivit√§t des Agenten nachl√§sst, da sich Eingabemuster √§ndern) und unerwartete Anfragen oder Situationen erkennen, die nicht in Ihren Testdaten enthalten waren. Sie bietet ein echtes Bild davon, wie sich der Agent in der Praxis verh√§lt.

Die Online-Bewertung umfasst oft das Sammeln von implizitem und explizitem Benutzerfeedback, wie bereits besprochen, und m√∂glicherweise das Durchf√ºhren von Shadow-Tests oder A/B-Tests (bei denen eine neue Version des Agenten parallel l√§uft, um sie mit der alten zu vergleichen). Die Herausforderung besteht darin, zuverl√§ssige Labels oder Bewertungen f√ºr Live-Interaktionen zu erhalten ‚Äì Sie k√∂nnten sich auf Benutzerfeedback oder nachgelagerte Metriken verlassen (z. B. ob der Benutzer auf das Ergebnis geklickt hat).

### Kombination der beiden

Online- und Offline-Bewertungen schlie√üen sich nicht gegenseitig aus; sie erg√§nzen sich stark. Erkenntnisse aus der Online-√úberwachung (z. B. neue Arten von Benutzeranfragen, bei denen der Agent schlecht abschneidet) k√∂nnen verwendet werden, um Offline-Testdatens√§tze zu erweitern und zu verbessern. Umgekehrt k√∂nnen Agenten, die in Offline-Tests gut abschneiden, mit gr√∂√üerem Vertrauen bereitgestellt und online √ºberwacht werden.

Tats√§chlich √ºbernehmen viele Teams einen Zyklus:

_offline bewerten -> bereitstellen -> online √ºberwachen -> neue Fehlerf√§lle sammeln -> zum Offline-Datensatz hinzuf√ºgen -> Agenten verfeinern -> wiederholen_.

## H√§ufige Probleme

Wenn Sie KI-Agenten in die Produktion bringen, k√∂nnen verschiedene Herausforderungen auftreten. Hier sind einige h√§ufige Probleme und m√∂gliche L√∂sungen:

| **Problem**    | **M√∂gliche L√∂sung**   |
| ------------- | ------------------ |
| KI-Agent f√ºhrt Aufgaben nicht konsistent aus | - Verfeinern Sie den Prompt, der dem KI-Agenten gegeben wird; seien Sie klar in den Zielen.<br>- Identifizieren Sie, ob das Aufteilen der Aufgaben in Unteraufgaben und deren Bearbeitung durch mehrere Agenten helfen kann. |
| KI-Agent ger√§t in Endlosschleifen  | - Stellen Sie sicher, dass Sie klare Abbruchbedingungen festlegen, damit der Agent wei√ü, wann der Prozess beendet werden soll. |

## H√§ufige Probleme und L√∂sungen

Hier sind einige h√§ufige Probleme, die bei der Arbeit mit AI-Agenten auftreten k√∂nnen, sowie Vorschl√§ge, wie man sie l√∂sen kann:

| **Problem**                                   | **L√∂sung**                                                                                     |
|-----------------------------------------------|------------------------------------------------------------------------------------------------|
| Modell liefert ungenaue Ergebnisse            | - √úberpr√ºfen Sie die Trainingsdaten auf Qualit√§t und Relevanz.<br>- Passen Sie die Eingabeaufforderungen (Prompts) an, um pr√§zisere Antworten zu erhalten.<br>- Verwenden Sie ein gr√∂√üeres Modell f√ºr komplexe Aufgaben, die mehr logisches Denken erfordern. |
| AI-Agenten-Toolaufrufe funktionieren nicht gut | - Testen und validieren Sie die Ausgabe des Tools au√üerhalb des Agentensystems.<br>- Verfeinern Sie die definierten Parameter, Prompts und die Benennung der Tools. |
| Multi-Agenten-System arbeitet inkonsistent    | - Verfeinern Sie die Prompts f√ºr jeden Agenten, um sicherzustellen, dass sie spezifisch und voneinander unterscheidbar sind.<br>- Erstellen Sie ein hierarchisches System mit einem "Routing"- oder Steuerungsagenten, der bestimmt, welcher Agent der richtige ist. |

Viele dieser Probleme k√∂nnen effektiver identifiziert werden, wenn eine gute Beobachtbarkeit vorhanden ist. Die zuvor besprochenen Traces und Metriken helfen dabei, genau zu bestimmen, wo im Agenten-Workflow Probleme auftreten, was das Debugging und die Optimierung erheblich erleichtert.

## Kostenmanagement

Hier sind einige Strategien, um die Kosten f√ºr den Einsatz von AI-Agenten in der Produktion zu verwalten:

**Verwendung kleinerer Modelle:** Kleine Sprachmodelle (Small Language Models, SLMs) k√∂nnen bei bestimmten agentischen Anwendungsf√§llen gut abschneiden und die Kosten erheblich senken. Wie bereits erw√§hnt, ist der Aufbau eines Bewertungssystems, um die Leistung im Vergleich zu gr√∂√üeren Modellen zu bestimmen und zu vergleichen, der beste Weg, um zu verstehen, wie gut ein SLM f√ºr Ihren Anwendungsfall geeignet ist. Erw√§gen Sie den Einsatz von SLMs f√ºr einfachere Aufgaben wie Intent-Klassifikation oder Parameterextraktion, w√§hrend gr√∂√üere Modelle f√ºr komplexe Denkaufgaben reserviert werden.

**Verwendung eines Router-Modells:** Eine √§hnliche Strategie besteht darin, eine Vielfalt von Modellen unterschiedlicher Gr√∂√üe zu nutzen. Sie k√∂nnen ein LLM/SLM oder eine serverlose Funktion verwenden, um Anfragen basierend auf ihrer Komplexit√§t an die am besten geeigneten Modelle weiterzuleiten. Dies hilft nicht nur, die Kosten zu senken, sondern stellt auch sicher, dass die Leistung f√ºr die richtigen Aufgaben optimiert wird. Beispielsweise k√∂nnen einfache Anfragen an kleinere, schnellere Modelle weitergeleitet werden, w√§hrend teure gro√üe Modelle nur f√ºr komplexe Denkaufgaben verwendet werden.

**Caching von Antworten:** Das Identifizieren h√§ufiger Anfragen und Aufgaben und das Bereitstellen der Antworten, bevor sie durch Ihr agentisches System laufen, ist eine gute M√∂glichkeit, das Volumen √§hnlicher Anfragen zu reduzieren. Sie k√∂nnen sogar einen Ablauf implementieren, um zu erkennen, wie √§hnlich eine Anfrage Ihren zwischengespeicherten Anfragen ist, indem Sie einfachere AI-Modelle verwenden. Diese Strategie kann die Kosten f√ºr h√§ufig gestellte Fragen oder g√§ngige Workflows erheblich senken.

## Schauen wir uns an, wie das in der Praxis funktioniert

Im [Beispiel-Notebook dieses Abschnitts](./code_samples/10_autogen_evaluation.ipynb) sehen wir Beispiele daf√ºr, wie wir Beobachtungstools nutzen k√∂nnen, um unsere Agenten zu √ºberwachen und zu bewerten.

### Noch Fragen zu AI-Agenten in der Produktion?

Treten Sie dem [Azure AI Foundry Discord](https://aka.ms/ai-agents/discord) bei, um sich mit anderen Lernenden auszutauschen, Sprechstunden zu besuchen und Ihre Fragen zu AI-Agenten beantwortet zu bekommen.

## Vorherige Lektion

[Metakognitions-Designmuster](../09-metacognition/README.md)

## N√§chste Lektion

[Agentische Protokolle](../11-agentic-protocols/README.md)

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.