<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-24T08:26:58+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "th"
}
-->
# เอเจนต์ AI ในการใช้งานจริง: การสังเกตการณ์และการประเมินผล

เมื่อเอเจนต์ AI เปลี่ยนจากต้นแบบทดลองไปสู่การใช้งานในโลกจริง ความสามารถในการเข้าใจพฤติกรรมของพวกเขา การติดตามผลการทำงาน และการประเมินผลอย่างเป็นระบบกลายเป็นสิ่งสำคัญ

## เป้าหมายการเรียนรู้

หลังจากจบบทเรียนนี้ คุณจะสามารถ:
- เข้าใจแนวคิดหลักของการสังเกตการณ์และการประเมินผลเอเจนต์
- ใช้เทคนิคเพื่อปรับปรุงประสิทธิภาพ ค่าใช้จ่าย และความสามารถของเอเจนต์
- รู้ว่าควรประเมินอะไรและอย่างไรในเอเจนต์ AI ของคุณ
- ควบคุมค่าใช้จ่ายเมื่อปรับใช้เอเจนต์ AI ในการใช้งานจริง
- ติดตั้งเครื่องมือสำหรับเอเจนต์ที่สร้างด้วย AutoGen

เป้าหมายคือการให้ความรู้ที่ช่วยเปลี่ยนเอเจนต์แบบ "กล่องดำ" ให้กลายเป็นระบบที่โปร่งใส จัดการได้ และเชื่อถือได้

_**หมายเหตุ:** การปรับใช้เอเจนต์ AI ที่ปลอดภัยและน่าเชื่อถือเป็นสิ่งสำคัญ ดูบทเรียน [การสร้างเอเจนต์ AI ที่น่าเชื่อถือ](./06-building-trustworthy-agents/README.md) เพื่อข้อมูลเพิ่มเติม_

## การติดตามและช่วงเวลา (Traces and Spans)

เครื่องมือสังเกตการณ์ เช่น [Langfuse](https://langfuse.com/) หรือ [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) มักแสดงการทำงานของเอเจนต์ในรูปแบบของการติดตาม (traces) และช่วงเวลา (spans)

- **Trace** หมายถึงงานของเอเจนต์ที่สมบูรณ์ตั้งแต่ต้นจนจบ (เช่น การจัดการคำถามของผู้ใช้)
- **Spans** คือขั้นตอนย่อยๆ ภายใน trace (เช่น การเรียกใช้โมเดลภาษา หรือการดึงข้อมูล)

![Trace tree in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

หากไม่มีการสังเกตการณ์ เอเจนต์ AI อาจดูเหมือน "กล่องดำ" ซึ่งสถานะภายในและเหตุผลของมันไม่ชัดเจน ทำให้ยากต่อการวินิจฉัยปัญหาหรือปรับปรุงประสิทธิภาพ แต่ด้วยการสังเกตการณ์ เอเจนต์จะกลายเป็น "กล่องแก้ว" ที่โปร่งใส ซึ่งเป็นสิ่งสำคัญสำหรับการสร้างความไว้วางใจและการทำให้มั่นใจว่าเอเจนต์ทำงานตามที่ตั้งใจไว้

## ทำไมการสังเกตการณ์จึงสำคัญในสภาพแวดล้อมการใช้งานจริง

การเปลี่ยนเอเจนต์ AI ไปสู่การใช้งานจริงนำมาซึ่งความท้าทายและข้อกำหนดใหม่ๆ การสังเกตการณ์จึงไม่ใช่แค่ "สิ่งที่ควรมี" แต่เป็นความสามารถที่สำคัญ:

*   **การแก้ไขข้อผิดพลาดและการวิเคราะห์สาเหตุ**: เมื่อเอเจนต์ล้มเหลวหรือให้ผลลัพธ์ที่ไม่คาดคิด เครื่องมือสังเกตการณ์จะให้ trace ที่จำเป็นสำหรับการระบุแหล่งที่มาของข้อผิดพลาด ซึ่งสำคัญอย่างยิ่งในเอเจนต์ที่ซับซ้อนซึ่งอาจเกี่ยวข้องกับการเรียกใช้ LLM หลายครั้ง การโต้ตอบกับเครื่องมือ และตรรกะตามเงื่อนไข
*   **การจัดการเวลาแฝงและค่าใช้จ่าย**: เอเจนต์ AI มักพึ่งพา LLM และ API ภายนอกที่มีค่าใช้จ่ายตามจำนวนโทเค็นหรือการเรียกใช้ การสังเกตการณ์ช่วยติดตามการเรียกใช้เหล่านี้อย่างแม่นยำ ช่วยระบุการดำเนินการที่ช้าเกินไปหรือมีค่าใช้จ่ายสูงเกินไป ทำให้ทีมสามารถปรับแต่ง prompt เลือกโมเดลที่มีประสิทธิภาพมากขึ้น หรือออกแบบเวิร์กโฟลว์ใหม่เพื่อจัดการค่าใช้จ่ายและมอบประสบการณ์ที่ดีให้กับผู้ใช้
*   **ความไว้วางใจ ความปลอดภัย และการปฏิบัติตามข้อกำหนด**: ในหลายแอปพลิเคชัน การทำให้มั่นใจว่าเอเจนต์ทำงานอย่างปลอดภัยและมีจริยธรรมเป็นสิ่งสำคัญ การสังเกตการณ์ให้เส้นทางการตรวจสอบการกระทำและการตัดสินใจของเอเจนต์ ซึ่งสามารถใช้เพื่อตรวจจับและลดปัญหา เช่น การโจมตีด้วย prompt การสร้างเนื้อหาที่เป็นอันตราย หรือการจัดการข้อมูลส่วนบุคคลที่ไม่เหมาะสม
*   **วงจรการปรับปรุงอย่างต่อเนื่อง**: ข้อมูลการสังเกตการณ์เป็นพื้นฐานของกระบวนการพัฒนาแบบวนซ้ำ โดยการติดตามว่าเอเจนต์ทำงานอย่างไรในโลกจริง ทีมสามารถระบุพื้นที่ที่ต้องปรับปรุง รวบรวมข้อมูลสำหรับการปรับแต่งโมเดล และตรวจสอบผลกระทบของการเปลี่ยนแปลง สร้างวงจรป้อนกลับที่ข้อมูลจากการประเมินออนไลน์แจ้งการทดลองและการปรับปรุงแบบออฟไลน์ นำไปสู่ประสิทธิภาพของเอเจนต์ที่ดีขึ้นเรื่อยๆ

## ตัวชี้วัดสำคัญที่ควรติดตาม

เพื่อเฝ้าดูและเข้าใจพฤติกรรมของเอเจนต์ ควรติดตามตัวชี้วัดและสัญญาณต่างๆ ซึ่งตัวชี้วัดที่เฉพาะเจาะจงอาจแตกต่างกันไปตามวัตถุประสงค์ของเอเจนต์ แต่มีบางตัวที่สำคัญในทุกกรณี

นี่คือตัวชี้วัดที่พบบ่อยที่สุดที่เครื่องมือสังเกตการณ์ติดตาม:

**เวลาแฝง (Latency):** เอเจนต์ตอบสนองเร็วแค่ไหน? เวลารอที่นานเกินไปส่งผลเสียต่อประสบการณ์ของผู้ใช้ ควรวัดเวลาแฝงสำหรับงานและขั้นตอนย่อยโดยการติดตามการทำงานของเอเจนต์ ตัวอย่างเช่น หากเอเจนต์ใช้เวลา 20 วินาทีสำหรับการเรียกใช้โมเดลทั้งหมด อาจเร่งความเร็วได้โดยใช้โมเดลที่เร็วขึ้นหรือเรียกใช้โมเดลแบบขนาน

**ค่าใช้จ่าย (Costs):** ค่าใช้จ่ายต่อการทำงานของเอเจนต์คือเท่าไร? เอเจนต์ AI พึ่งพาการเรียกใช้ LLM ที่มีค่าใช้จ่ายตามจำนวนโทเค็นหรือ API ภายนอก การใช้เครื่องมือบ่อยครั้งหรือการส่ง prompt หลายครั้งอาจเพิ่มค่าใช้จ่ายอย่างรวดเร็ว ตัวอย่างเช่น หากเอเจนต์เรียกใช้ LLM ห้าครั้งเพื่อปรับปรุงคุณภาพเพียงเล็กน้อย คุณต้องประเมินว่าค่าใช้จ่ายนั้นคุ้มค่าหรือไม่ หรือสามารถลดจำนวนการเรียกใช้หรือใช้โมเดลที่ถูกกว่าได้

**ข้อผิดพลาดในการร้องขอ (Request Errors):** เอเจนต์ล้มเหลวในการร้องขอกี่ครั้ง? ซึ่งอาจรวมถึงข้อผิดพลาดของ API หรือการเรียกใช้เครื่องมือที่ล้มเหลว เพื่อทำให้เอเจนต์ของคุณมีความทนทานมากขึ้นในสภาพแวดล้อมการใช้งานจริง คุณสามารถตั้งค่าการสำรองข้อมูลหรือการลองใหม่ได้ เช่น หากผู้ให้บริการ LLM A ล่ม คุณสามารถเปลี่ยนไปใช้ผู้ให้บริการ LLM B เป็นตัวสำรอง

**ความคิดเห็นจากผู้ใช้ (User Feedback):** การประเมินโดยตรงจากผู้ใช้ให้ข้อมูลเชิงลึกที่มีค่า ซึ่งอาจรวมถึงการให้คะแนนแบบชัดเจน (👍/👎, ⭐1-5 ดาว) หรือความคิดเห็นเป็นข้อความ หากได้รับความคิดเห็นเชิงลบอย่างต่อเนื่อง ควรถือเป็นสัญญาณว่าเอเจนต์ทำงานไม่เป็นไปตามที่คาดหวัง

**ความคิดเห็นโดยนัยจากผู้ใช้ (Implicit User Feedback):** พฤติกรรมของผู้ใช้ให้ข้อมูลเชิงอ้อมแม้ไม่มีการให้คะแนนชัดเจน เช่น การถามคำถามใหม่ทันที การถามซ้ำ หรือการคลิกปุ่มลองใหม่ ตัวอย่างเช่น หากเห็นว่าผู้ใช้ถามคำถามเดิมซ้ำๆ นี่เป็นสัญญาณว่าเอเจนต์ทำงานไม่เป็นไปตามที่คาดหวัง

**ความแม่นยำ (Accuracy):** เอเจนต์ให้ผลลัพธ์ที่ถูกต้องหรือเป็นที่ต้องการบ่อยแค่ไหน? คำจำกัดความของความแม่นยำอาจแตกต่างกันไป (เช่น ความถูกต้องในการแก้ปัญหา ความแม่นยำในการดึงข้อมูล ความพึงพอใจของผู้ใช้) ขั้นตอนแรกคือการกำหนดว่า "ความสำเร็จ" สำหรับเอเจนต์ของคุณคืออะไร

**ตัวชี้วัดการประเมินอัตโนมัติ (Automated Evaluation Metrics):** คุณสามารถตั้งค่าการประเมินอัตโนมัติได้ เช่น ใช้ LLM เพื่อให้คะแนนผลลัพธ์ของเอเจนต์ว่ามีประโยชน์ ถูกต้อง หรือไม่ นอกจากนี้ยังมีไลบรารีโอเพ่นซอร์สหลายตัวที่ช่วยให้คุณให้คะแนนแง่มุมต่างๆ ของเอเจนต์ เช่น [RAGAS](https://docs.ragas.io/) สำหรับเอเจนต์ RAG หรือ [LLM Guard](https://llm-guard.com/) เพื่อตรวจจับภาษาที่เป็นอันตรายหรือการโจมตีด้วย prompt

ในทางปฏิบัติ การใช้ตัวชี้วัดเหล่านี้ร่วมกันจะให้ภาพรวมที่ครอบคลุมที่สุดเกี่ยวกับสุขภาพของเอเจนต์ AI ในบทเรียนนี้ เราจะสาธิตตัวอย่างจริงใน [สมุดบันทึกตัวอย่าง](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) แต่ก่อนอื่นเราจะเรียนรู้เกี่ยวกับเวิร์กโฟลว์การประเมินทั่วไป

## การติดตั้งเครื่องมือในเอเจนต์ของคุณ

เพื่อรวบรวมข้อมูล trace คุณจะต้องติดตั้งเครื่องมือในโค้ดของคุณ เป้าหมายคือการทำให้โค้ดเอเจนต์สามารถส่ง trace และตัวชี้วัดที่สามารถจับ ประมวลผล และแสดงผลโดยแพลตฟอร์มสังเกตการณ์

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) ได้กลายเป็นมาตรฐานอุตสาหกรรมสำหรับการสังเกตการณ์ LLM โดยให้ API, SDK และเครื่องมือสำหรับการสร้าง การรวบรวม และการส่งออกข้อมูล telemetry

มีไลบรารีการติดตั้งเครื่องมือหลายตัวที่ครอบคลุมเฟรมเวิร์กเอเจนต์ที่มีอยู่และทำให้ง่ายต่อการส่งออก span ของ OpenTelemetry ไปยังเครื่องมือสังเกตการณ์ ด้านล่างคือตัวอย่างการติดตั้งเครื่องมือในเอเจนต์ AutoGen ด้วย [ไลบรารี OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

สมุดบันทึกตัวอย่างในบทนี้จะแสดงวิธีการติดตั้งเครื่องมือในเอเจนต์ AutoGen ของคุณ

**การสร้าง Span ด้วยตนเอง:** แม้ว่าไลบรารีการติดตั้งเครื่องมือจะให้พื้นฐานที่ดี แต่บ่อยครั้งที่ต้องการข้อมูลที่ละเอียดหรือกำหนดเองมากขึ้น คุณสามารถสร้าง span ด้วยตนเองเพื่อเพิ่มตรรกะแอปพลิเคชันที่กำหนดเองได้ ตัวอย่างการสร้าง trace และ span ด้วยตนเองโดยใช้ [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## การประเมินเอเจนต์

การสังเกตการณ์ให้ตัวชี้วัด แต่การประเมินคือกระบวนการวิเคราะห์ข้อมูลนั้น (และการทดสอบ) เพื่อพิจารณาว่าเอเจนต์ AI ทำงานได้ดีเพียงใดและสามารถปรับปรุงได้อย่างไร กล่าวอีกนัยหนึ่ง เมื่อคุณมี trace และตัวชี้วัดเหล่านั้นแล้ว คุณจะใช้มันในการตัดสินเอเจนต์และตัดสินใจได้อย่างไร?

การประเมินผลอย่างสม่ำเสมอเป็นสิ่งสำคัญ เพราะเอเจนต์ AI มักไม่แน่นอนและสามารถเปลี่ยนแปลงได้ (ผ่านการอัปเดตหรือพฤติกรรมโมเดลที่เปลี่ยนไป) – หากไม่มีการประเมิน คุณจะไม่รู้ว่า "เอเจนต์อัจฉริยะ" ของคุณทำงานได้ดีจริงหรือไม่ หรือถ้ามันถดถอย

มีการประเมินผลสองประเภทสำหรับเอเจนต์ AI: **การประเมินออนไลน์** และ **การประเมินออฟไลน์** ทั้งสองมีคุณค่าและเสริมซึ่งกันและกัน โดยปกติเราจะเริ่มต้นด้วยการประเมินออฟไลน์ เนื่องจากเป็นขั้นตอนขั้นต่ำที่จำเป็นก่อนการปรับใช้เอเจนต์ใดๆ

### การประเมินออฟไลน์

![Dataset items in Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

การประเมินนี้เกี่ยวข้องกับการประเมินเอเจนต์ในสภาพแวดล้อมที่ควบคุมได้ โดยปกติจะใช้ชุดข้อมูลทดสอบ ไม่ใช่คำถามจากผู้ใช้จริง คุณใช้ชุดข้อมูลที่คัดสรรมา ซึ่งคุณทราบผลลัพธ์ที่คาดหวังหรือพฤติกรรมที่ถูกต้อง และจากนั้นรันเอเจนต์ของคุณบนชุดข้อมูลเหล่านั้น

ตัวอย่างเช่น หากคุณสร้างเอเจนต์แก้ปัญหาโจทย์คณิตศาสตร์ คุณอาจมี [ชุดข้อมูลทดสอบ](https://huggingface.co/datasets/gsm8k) ที่มีปัญหา 100 ข้อพร้อมคำตอบที่ทราบ การประเมินออฟไลน์มักทำในระหว่างการพัฒนา (และสามารถเป็นส่วนหนึ่งของ CI/CD pipeline) เพื่อตรวจสอบการปรับปรุงหรือป้องกันการถดถอย ข้อดีคือมัน **ทำซ้ำได้และคุณสามารถรับตัวชี้วัดความแม่นยำที่ชัดเจนได้ เนื่องจากคุณมีคำตอบที่ถูกต้อง**

ความท้าทายสำคัญของการประเมินออฟไลน์คือการทำให้แน่ใจว่าชุดข้อมูลทดสอบของคุณครอบคลุมและยังคงเกี่ยวข้อง – เอเจนต์อาจทำงานได้ดีในชุดทดสอบที่กำหนด แต่พบคำถามที่แตกต่างกันมากในสภาพแวดล้อมการใช้งานจริง ดังนั้นคุณควรอัปเดตชุดทดสอบด้วยกรณีขอบใหม่และตัวอย่างที่สะท้อนสถานการณ์ในโลกจริง

### การประเมินออนไลน์

![Observability metrics overview](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

การประเมินนี้หมายถึงการประเมินเอเจนต์ในสภาพแวดล้อมจริง โดยใช้ข้อมูลจากการใช้งานจริงในโลกจริง การประเมินออนไลน์เกี่ยวข้องกับการติดตามผลการทำงานของเอเจนต์ในปฏิสัมพันธ์กับผู้ใช้จริงและการวิเคราะห์ผลลัพธ์อย่างต่อเนื่อง

ตัวอย่างเช่น คุณอาจติดตามอัตราความสำเร็จ คะแนนความพึงพอใจของผู้ใช้ หรือเมตริกอื่นๆ บนทราฟฟิกจริง ข้อดีของการประเมินออนไลน์คือมัน **จับสิ่งที่คุณอาจไม่คาดคิดในห้องทดลอง** – คุณสามารถสังเกตการเปลี่ยนแปลงของโมเดลเมื่อเวลาผ่านไป (หากประสิทธิภาพของเอเจนต์ลดลงเมื่อรูปแบบอินพุตเปลี่ยนไป) และจับคำถามหรือสถานการณ์ที่ไม่คาดคิดซึ่งไม่ได้อยู่ในข้อมูลทดสอบของคุณ

การประเมินออนไลน์มักเกี่ยวข้องกับการรวบรวมความคิดเห็นโดยนัยและชัดเจนจากผู้ใช้ และอาจรวมถึงการทดสอบแบบเงาหรือการทดสอบ A/B (ที่เวอร์ชันใหม่ของเอเจนต์ทำงานคู่ขนานเพื่อเปรียบเทียบกับเวอร์ชันเก่า)

### การรวมทั้งสองวิธี

การประเมินออนไลน์และออฟไลน์ไม่ได้แยกจากกัน แต่เสริมซึ่งกันและกัน ข้อมูลเชิงลึกจากการติดตามออนไลน์ (เช่น ประเภทใหม่ของคำถามผู้ใช้ที่เอเจนต์ทำงานได้ไม่ดี) สามารถใช้เพื่อเพิ่มและปรับปรุงชุดข้อมูลทดสอบออฟไลน์ ในทางกลับกัน เอเจนต์ที่ทำงานได้ดีในการทดสอบออฟไลน์สามารถปรับใช้และติดตามออนไลน์ได้อย่างมั่นใจมากขึ้น

ในความเป็นจริง หลายทีมใช้กระบวนการวนซ้ำ:

_ประเมินออฟไลน์ -> ปรับใช้ -> ติดตามออนไลน์ -> รวบรวมกรณีล้มเหลวใหม่ -> เพิ่มในชุดข้อมูลออฟไลน์ -> ปรับปรุงเอเจนต์ -> ทำซ้ำ_

## ปัญหาที่พบบ่อย

เมื่อคุณปรับใช้เอเจนต์ AI ในการใช้งานจริง คุณอาจพบปัญหาต่างๆ ต่อไปนี้คือตัวอย่างปัญหาที่พบบ่อยและวิธีแก้ไขที่เป็นไปได้:

| **ปัญหา**    | **วิธีแก้ไขที่เป็นไปได้**   |
| ------------- | ------------------ |
| เอเจนต์ AI ทำงานไม่สม่ำเสมอ | - ปรับปรุง prompt ที่ให้กับเอเจนต์ AI; ระบุวัตถุประสงค์ให้ชัดเจน<br>- พ

- ปรับแต่งพารามิเตอร์, คำสั่ง, และการตั้งชื่อเครื่องมือให้เหมาะสม |
| ระบบ Multi-Agent ทำงานไม่สม่ำเสมอ | - ปรับแต่งคำสั่งที่ให้กับแต่ละเอเจนต์เพื่อให้มีความเฉพาะเจาะจงและแตกต่างกันอย่างชัดเจน<br>- สร้างระบบลำดับชั้นโดยใช้ "routing" หรือเอเจนต์ควบคุมเพื่อกำหนดว่าเอเจนต์ใดเหมาะสมที่สุด |

ปัญหาหลายอย่างเหล่านี้สามารถระบุได้อย่างมีประสิทธิภาพมากขึ้นเมื่อมีระบบการสังเกตการณ์ (observability) ข้อมูลการติดตาม (traces) และเมตริกที่เราได้พูดถึงก่อนหน้านี้ช่วยชี้จุดที่เกิดปัญหาในกระบวนการทำงานของเอเจนต์ ทำให้การดีบักและการปรับปรุงประสิทธิภาพง่ายขึ้นมาก

## การจัดการต้นทุน

นี่คือกลยุทธ์บางอย่างสำหรับการจัดการต้นทุนในการนำ AI agents ไปใช้งานในระบบจริง:

**การใช้โมเดลขนาดเล็ก:** Small Language Models (SLMs) สามารถทำงานได้ดีในบางกรณีการใช้งานแบบเอเจนต์ และช่วยลดต้นทุนได้อย่างมาก ดังที่ได้กล่าวไว้ก่อนหน้านี้ การสร้างระบบประเมินผลเพื่อเปรียบเทียบประสิทธิภาพระหว่างโมเดลขนาดเล็กและโมเดลขนาดใหญ่เป็นวิธีที่ดีที่สุดในการเข้าใจว่า SLM จะทำงานได้ดีเพียงใดในกรณีการใช้งานของคุณ ลองพิจารณาใช้ SLM สำหรับงานที่ง่าย เช่น การจัดหมวดหมู่เจตนา (intent classification) หรือการดึงพารามิเตอร์ ในขณะที่สงวนโมเดลขนาดใหญ่ไว้สำหรับงานที่ต้องใช้การวิเคราะห์ซับซ้อน

**การใช้ Router Model:** กลยุทธ์ที่คล้ายกันคือการใช้โมเดลที่หลากหลายทั้งในด้านขนาดและความสามารถ คุณสามารถใช้ LLM/SLM หรือฟังก์ชันแบบ serverless เพื่อกำหนดเส้นทางคำขอไปยังโมเดลที่เหมาะสมที่สุดตามความซับซ้อน วิธีนี้จะช่วยลดต้นทุนในขณะเดียวกันก็ยังคงประสิทธิภาพในงานที่เหมาะสม ตัวอย่างเช่น กำหนดเส้นทางคำถามง่าย ๆ ไปยังโมเดลที่เล็กและเร็วกว่า และใช้โมเดลขนาดใหญ่ที่มีต้นทุนสูงสำหรับงานที่ต้องการการวิเคราะห์ซับซ้อนเท่านั้น

**การแคชคำตอบ:** การระบุคำขอและงานที่พบบ่อย และจัดเตรียมคำตอบไว้ล่วงหน้าก่อนที่จะผ่านระบบเอเจนต์ของคุณ เป็นวิธีที่ดีในการลดปริมาณคำขอที่คล้ายกัน คุณยังสามารถสร้างกระบวนการเพื่อระบุว่าคำขอใหม่มีความคล้ายคลึงกับคำขอที่แคชไว้มากน้อยเพียงใดโดยใช้โมเดล AI ที่พื้นฐานกว่า กลยุทธ์นี้สามารถลดต้นทุนได้อย่างมากสำหรับคำถามที่พบบ่อยหรือกระบวนการทำงานที่ซ้ำ ๆ

## มาดูตัวอย่างการใช้งานจริง

ใน [ตัวอย่างโน้ตบุ๊กของส่วนนี้](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) เราจะเห็นตัวอย่างของการใช้เครื่องมือสังเกตการณ์เพื่อติดตามและประเมินผลเอเจนต์ของเรา

## บทเรียนก่อนหน้า

[Metacognition Design Pattern](../09-metacognition/README.md)

## บทเรียนถัดไป

[MCP](../11-mcp/README.md)

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาต้นทางควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามนุษย์มืออาชีพ เราจะไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่ผิดพลาดซึ่งเกิดจากการใช้การแปลนี้