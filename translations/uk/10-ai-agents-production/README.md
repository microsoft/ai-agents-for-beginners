<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cdfd0acc8592c1af14f8637833450375",
  "translation_date": "2025-08-30T09:47:35+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "uk"
}
-->
# AI-агенти у виробництві: Спостережуваність та оцінка

[![AI-агенти у виробництві](../../../translated_images/lesson-10-thumbnail.2b79a30773db093e0b4fb47aaa618069e0afb4745fad4836526cf51df87f9ac9.uk.png)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

Коли AI-агенти переходять від експериментальних прототипів до реальних застосувань, стає важливо розуміти їхню поведінку, відстежувати їхню продуктивність і систематично оцінювати їхні результати.

## Цілі навчання

Після завершення цього уроку ви знатимете/зрозумієте:
- Основні концепції спостережуваності та оцінки агентів
- Техніки покращення продуктивності, витрат і ефективності агентів
- Що і як систематично оцінювати у ваших AI-агентах
- Як контролювати витрати при розгортанні AI-агентів у виробництві
- Як інструментувати агентів, створених за допомогою AutoGen

Мета — надати вам знання, які допоможуть перетворити ваших "чорних ящиків" на прозорі, керовані та надійні системи.

_**Примітка:** Важливо розгортати AI-агентів, які є безпечними та надійними. Ознайомтеся з уроком [Створення надійних AI-агентів](./06-building-trustworthy-agents/README.md)._

## Трейси та спани

Інструменти спостережуваності, такі як [Langfuse](https://langfuse.com/) або [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry), зазвичай представляють виконання агентів у вигляді трейсів і спанів.

- **Трейс** представляє повне завдання агента від початку до кінця (наприклад, обробка запиту користувача).
- **Спани** — це окремі кроки в межах трейсу (наприклад, виклик мовної моделі або отримання даних).

![Дерево трейсів у Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Без спостережуваності AI-агент може здаватися "чорним ящиком" — його внутрішній стан і логіка залишаються непрозорими, що ускладнює діагностику проблем або оптимізацію продуктивності. Завдяки спостережуваності агенти стають "скляними ящиками", забезпечуючи прозорість, яка є життєво важливою для побудови довіри та забезпечення їхньої роботи відповідно до очікувань.

## Чому спостережуваність важлива у виробничих середовищах

Перехід AI-агентів у виробничі середовища створює нові виклики та вимоги. Спостережуваність більше не є "приємною опцією", а стає критичною можливістю:

*   **Відлагодження та аналіз першопричин:** Коли агент зазнає невдачі або видає несподіваний результат, інструменти спостережуваності надають трейси, необхідні для визначення джерела помилки. Це особливо важливо для складних агентів, які можуть включати кілька викликів LLM, взаємодії з інструментами та умовну логіку.
*   **Управління затримками та витратами:** AI-агенти часто покладаються на LLM та інші зовнішні API, які оплачуються за токен або виклик. Спостережуваність дозволяє точно відстежувати ці виклики, допомагаючи виявляти операції, які є надто повільними або дорогими. Це дозволяє командам оптимізувати підказки, вибирати більш ефективні моделі або переробляти робочі процеси для управління операційними витратами та забезпечення гарного користувацького досвіду.
*   **Довіра, безпека та відповідність:** У багатьох застосуваннях важливо забезпечити, щоб агенти діяли безпечно та етично. Спостережуваність надає журнал дій і рішень агента. Це може бути використано для виявлення та усунення таких проблем, як ін'єкція підказок, генерація шкідливого контенту або неправильне поводження з персональною інформацією (PII). Наприклад, ви можете переглядати трейси, щоб зрозуміти, чому агент надав певну відповідь або використав конкретний інструмент.
*   **Цикли безперервного вдосконалення:** Дані спостережуваності є основою ітеративного процесу розробки. Відстежуючи, як агенти працюють у реальному світі, команди можуть визначати області для покращення, збирати дані для доопрацювання моделей і перевіряти вплив змін. Це створює цикл зворотного зв’язку, де виробничі інсайти з онлайн-оцінки інформують офлайн-експерименти та вдосконалення, що призводить до поступового покращення продуктивності агентів.

## Основні метрики для відстеження

Для моніторингу та розуміння поведінки агентів слід відстежувати низку метрик і сигналів. Хоча конкретні метрики можуть варіюватися залежно від призначення агента, деякі з них є універсально важливими.

Ось деякі з найпоширеніших метрик, які відстежують інструменти спостережуваності:

**Затримка:** Як швидко відповідає агент? Тривалі очікування негативно впливають на досвід користувача. Ви повинні вимірювати затримку для завдань і окремих кроків, відстежуючи виконання агентів. Наприклад, агент, який витрачає 20 секунд на всі виклики моделі, може бути прискорений за допомогою швидшої моделі або паралельного виконання викликів.

**Витрати:** Яка вартість одного виконання агента? AI-агенти покладаються на виклики LLM, які оплачуються за токен, або зовнішні API. Часте використання інструментів або численні підказки можуть швидко збільшити витрати. Наприклад, якщо агент викликає LLM п’ять разів для незначного покращення якості, потрібно оцінити, чи виправдані витрати, чи можна зменшити кількість викликів або використовувати дешевшу модель. Моніторинг у реальному часі також допомагає виявляти несподівані сплески (наприклад, помилки, що викликають надмірні цикли API).

**Помилки запитів:** Скільки запитів агента зазнали невдачі? Це може включати помилки API або невдалі виклики інструментів. Щоб зробити вашого агента більш стійким у виробництві, ви можете налаштувати резервні варіанти або повторні спроби. Наприклад, якщо постачальник LLM A недоступний, ви перемикаєтеся на постачальника LLM B як резерв.

**Зворотний зв’язок від користувачів:** Реалізація прямої оцінки користувачами надає цінну інформацію. Це може включати явні оцінки (👍палець вгору/👎вниз, ⭐1-5 зірок) або текстові коментарі. Постійний негативний зворотний зв’язок має вас насторожити, оскільки це ознака того, що агент працює не так, як очікувалося.

**Непрямий зворотний зв’язок від користувачів:** Поведінка користувачів надає непрямий зворотний зв’язок навіть без явних оцінок. Це може включати негайне перефразування запитання, повторні запити або натискання кнопки повтору. Наприклад, якщо ви бачите, що користувачі постійно задають одне й те саме запитання, це ознака того, що агент працює не так, як очікувалося.

**Точність:** Як часто агент видає правильні або бажані результати? Визначення точності може варіюватися (наприклад, правильність розв’язання задач, точність отримання інформації, задоволеність користувачів). Перший крок — визначити, що означає успіх для вашого агента. Ви можете відстежувати точність за допомогою автоматизованих перевірок, оцінок або міток виконання завдань. Наприклад, позначати трейси як "успішні" або "невдалі".

**Автоматизовані метрики оцінки:** Ви також можете налаштувати автоматизовані оцінки. Наприклад, ви можете використовувати LLM для оцінки результатів агента, наприклад, чи є вони корисними, точними чи ні. Існує також кілька бібліотек з відкритим кодом, які допомагають оцінювати різні аспекти агента. Наприклад, [RAGAS](https://docs.ragas.io/) для RAG-агентів або [LLM Guard](https://llm-guard.com/) для виявлення шкідливої мови або ін’єкції підказок.

На практиці поєднання цих метрик забезпечує найкраще охоплення стану здоров’я AI-агента. У [прикладному ноутбуці](./code_samples/10_autogen_evaluation.ipynb) цього розділу ми покажемо, як ці метрики виглядають на реальних прикладах, але спочатку дізнаємося, як виглядає типовий робочий процес оцінки.

## Інструментування вашого агента

Щоб збирати дані про трейси, вам потрібно інструментувати ваш код. Мета полягає в тому, щоб інструментувати код агента для створення трейсів і метрик, які можуть бути зібрані, оброблені та візуалізовані платформою спостережуваності.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) став галузевим стандартом для спостережуваності LLM. Він надає набір API, SDK та інструментів для генерації, збору та експорту телеметричних даних.

Існує багато бібліотек інструментування, які обгортають існуючі фреймворки агентів і спрощують експорт спанів OpenTelemetry до інструменту спостережуваності. Нижче наведено приклад інструментування агента AutoGen за допомогою бібліотеки інструментування [OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

[Прикладний ноутбук](./code_samples/10_autogen_evaluation.ipynb) у цьому розділі продемонструє, як інструментувати вашого агента AutoGen.

**Ручне створення спанів:** Хоча бібліотеки інструментування забезпечують хорошу базу, часто виникають випадки, коли потрібна більш детальна або спеціальна інформація. Ви можете вручну створювати спани, щоб додати власну логіку застосування. Більш того, вони можуть збагачувати автоматично або вручну створені спани спеціальними атрибутами (також відомими як теги або метадані). Ці атрибути можуть включати бізнес-специфічні дані, проміжні обчислення або будь-який контекст, який може бути корисним для відлагодження або аналізу, наприклад, `user_id`, `session_id` або `model_version`.

Приклад створення трейсів і спанів вручну за допомогою [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Оцінка агентів

Спостережуваність надає нам метрики, але оцінка — це процес аналізу цих даних (і проведення тестів), щоб визначити, наскільки добре працює AI-агент і як його можна покращити. Іншими словами, як тільки у вас є ці трейси та метрики, як ви використовуєте їх для оцінки агента та прийняття рішень?

Регулярна оцінка важлива, оскільки AI-агенти часто є недетермінованими і можуть змінюватися (через оновлення або дрейф поведінки моделі) – без оцінки ви не дізнаєтеся, чи ваш "розумний агент" дійсно виконує свою роботу добре, чи він регресує.

Існує дві категорії оцінки AI-агентів: **офлайн-оцінка** та **онлайн-оцінка**. Обидві є цінними і доповнюють одна одну. Зазвичай ми починаємо з офлайн-оцінки, оскільки це мінімально необхідний крок перед розгортанням будь-якого агента.

### Офлайн-оцінка

![Елементи набору даних у Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Це передбачає оцінку агента в контрольованому середовищі, зазвичай із використанням тестових наборів даних, а не запитів від реальних користувачів. Ви використовуєте підготовлені набори даних, де ви знаєте очікуваний результат або правильну поведінку, а потім запускаєте вашого агента на них.

Наприклад, якщо ви створили агента для розв’язання математичних задач, у вас може бути [тестовий набір даних](https://huggingface.co/datasets/gsm8k) із 100 задачами з відомими відповідями. Офлайн-оцінка часто проводиться під час розробки (і може бути частиною CI/CD-пайплайнів), щоб перевірити покращення або запобігти регресіям. Перевага в тому, що це **повторювано, і ви можете отримати чіткі метрики точності, оскільки у вас є еталонні дані**. Ви також можете імітувати запити користувачів і вимірювати відповіді агента щодо ідеальних відповідей або використовувати автоматизовані метрики, як описано вище.

Основна проблема офлайн-оцінки — забезпечити, щоб ваш тестовий набір даних був всеосяжним і залишався актуальним – агент може добре працювати на фіксованому тестовому наборі, але стикатися з дуже різними запитами у виробництві. Тому слід оновлювати тестові набори новими крайніми випадками та прикладами, які відображають реальні сценарії. Корисно мати суміш невеликих "швидких тестів" і більших наборів для оцінки: невеликі набори для швидких перевірок і більші для ширших метрик продуктивності.

### Онлайн-оцінка

![Огляд метрик спостережуваності](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Це стосується оцінки агента в реальному, живому середовищі, тобто під час фактичного використання у виробництві. Онлайн-оцінка передбачає постійний моніторинг продуктивності агента на основі реальних взаємодій користувачів і аналіз результатів.

Наприклад, ви можете відстежувати рівень успішності, оцінки задоволеності користувачів або інші метрики на основі живого трафіку. Перевага онлайн-оцінки в тому, що вона **виявляє речі, які ви могли не передбачити в лабораторних умовах** – ви можете спостерігати дрейф моделі з часом (якщо ефективність агента знижується через зміну шаблонів введення) і виявляти несподівані запити або ситуації, яких не було у ваших тестових даних. Вона надає справжню картину того, як агент поводиться в реальних умовах.

Онлайн-оцінка часто включає збір непрямого та прямого зворотного зв’язку від користувачів, як обговорювалося раніше, а також, можливо, проведення тіньових тестів або A/B-тестів (де нова версія агента працює паралельно для порівняння зі старою). Проблема в тому, що може бути складно отримати надійні мітки або оцінки для живих взаємодій – ви можете покладатися на зворотний зв’язок від користувачів або метрики нижчого рівня (наприклад, чи натиснув користувач на результат).

### Поєднання двох підходів

Онлайн- та офлайн-оцінки не є взаємовиключ

## Вирішення проблем із продуктивністю

### Поширені проблеми та їх вирішення

| **Проблема**                                   | **Рішення**                                                                                     |
|------------------------------------------------|-------------------------------------------------------------------------------------------------|
| Модель не відповідає очікуванням               | - Перевірте, чи правильно визначені параметри та підказки.<br>- Використовуйте більшу модель для складних завдань. |
| Виклики інструментів AI Agent працюють некоректно | - Тестуйте та перевіряйте результати інструментів поза системою агентів.<br>- Уточніть параметри, підказки та назви інструментів. |
| Система з кількома агентами працює нестабільно | - Уточніть підказки для кожного агента, щоб вони були конкретними та відрізнялися один від одного.<br>- Побудуйте ієрархічну систему з використанням "маршрутизуючого" або контрольного агента для визначення правильного агента. |

Багато з цих проблем можна ефективніше виявити за допомогою інструментів спостереження. Траси та метрики, які ми обговорювали раніше, допомагають точно визначити, на якому етапі робочого процесу агента виникають проблеми, що значно спрощує налагодження та оптимізацію.

## Управління витратами

Ось кілька стратегій для управління витратами на розгортання AI-агентів у виробництві:

**Використання менших моделей:** Малі мовні моделі (SLM) можуть добре працювати для певних агентських сценаріїв і значно знижують витрати. Як згадувалося раніше, створення системи оцінки для визначення та порівняння продуктивності з більшими моделями — найкращий спосіб зрозуміти, наскільки добре SLM підходить для вашого випадку використання. Розгляньте використання SLM для простих завдань, таких як класифікація намірів або вилучення параметрів, залишаючи більші моделі для складних завдань, що потребують міркувань.

**Використання маршрутизуючої моделі:** Схожа стратегія полягає у використанні різноманітних моделей різних розмірів. Ви можете використовувати LLM/SLM або безсерверну функцію для маршрутизації запитів залежно від їх складності до найбільш відповідних моделей. Це також допоможе знизити витрати, забезпечуючи при цьому продуктивність для відповідних завдань. Наприклад, прості запити можна направляти до менших, швидших моделей, а дорогі великі моделі використовувати лише для складних завдань.

**Кешування відповідей:** Визначення поширених запитів і завдань та надання відповідей до того, як вони проходять через вашу агентську систему, є хорошим способом зменшення обсягу схожих запитів. Ви навіть можете реалізувати процес для визначення схожості запиту з кешованими запитами за допомогою більш базових AI-моделей. Ця стратегія може значно знизити витрати для часто задаваних питань або поширених робочих процесів.

## Давайте подивимося, як це працює на практиці

У [прикладному ноутбуці цього розділу](./code_samples/10_autogen_evaluation.ipynb) ми побачимо приклади того, як можна використовувати інструменти спостереження для моніторингу та оцінки роботи агента.

### Є ще питання про AI-агентів у виробництві?

Приєднуйтесь до [Azure AI Foundry Discord](https://aka.ms/ai-agents/discord), щоб поспілкуватися з іншими учасниками, відвідати години консультацій та отримати відповіді на ваші запитання про AI-агентів.

## Попередній урок

[Шаблон метакогніції](../09-metacognition/README.md)

## Наступний урок

[Агентські протоколи](../11-agentic-protocols/README.md)

---

**Відмова від відповідальності**:  
Цей документ був перекладений за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ на його рідній мові слід вважати авторитетним джерелом. Для критичної інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникають внаслідок використання цього перекладу.