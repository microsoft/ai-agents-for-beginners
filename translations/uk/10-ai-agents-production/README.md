<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-24T09:18:28+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "uk"
}
-->
# AI-агенти у виробництві: Спостережуваність та оцінка

Коли AI-агенти переходять від експериментальних прототипів до реальних застосувань, важливо мати можливість розуміти їхню поведінку, контролювати їхню продуктивність та систематично оцінювати їхні результати.

## Цілі навчання

Після завершення цього уроку ви знатимете/розумітимете:
- Основні концепції спостережуваності та оцінки агентів
- Техніки покращення продуктивності, витрат та ефективності агентів
- Що і як систематично оцінювати у ваших AI-агентах
- Як контролювати витрати при впровадженні AI-агентів у виробництво
- Як інструментувати агентів, створених за допомогою AutoGen

Мета полягає в тому, щоб надати вам знання для перетворення ваших "чорних ящиків" у прозорі, керовані та надійні системи.

_**Примітка:** Важливо впроваджувати AI-агенти, які є безпечними та надійними. Ознайомтеся з уроком [Створення надійних AI-агентів](./06-building-trustworthy-agents/README.md)._

## Трейси та спани

Інструменти спостережуваності, такі як [Langfuse](https://langfuse.com/) або [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry), зазвичай представляють виконання агентів у вигляді трейсів та спанів.

- **Трейс** представляє повне завдання агента від початку до кінця (наприклад, обробка запиту користувача).
- **Спани** — це окремі кроки всередині трейсу (наприклад, виклик мовної моделі або отримання даних).

![Дерево трейсів у Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Без спостережуваності AI-агент може здаватися "чорним ящиком" — його внутрішній стан і логіка залишаються непрозорими, що ускладнює діагностику проблем або оптимізацію продуктивності. Завдяки спостережуваності агенти стають "скляними ящиками", забезпечуючи прозорість, яка є важливою для створення довіри та гарантування їхньої роботи відповідно до очікувань.

## Чому спостережуваність важлива у виробничих середовищах

Перехід AI-агентів у виробничі середовища створює новий набір викликів і вимог. Спостережуваність більше не є "приємною опцією", а стає критичною можливістю:

*   **Відлагодження та аналіз причин помилок**: Коли агент зазнає невдачі або видає несподіваний результат, інструменти спостережуваності надають трейси, необхідні для визначення джерела помилки. Це особливо важливо для складних агентів, які можуть включати кілька викликів LLM, взаємодії з інструментами та умовну логіку.
*   **Управління затримками та витратами**: AI-агенти часто покладаються на LLM та інші зовнішні API, які оплачуються за токен або виклик. Спостережуваність дозволяє точно відстежувати ці виклики, допомагаючи визначити операції, які є надто повільними або дорогими. Це дозволяє командам оптимізувати запити, вибирати більш ефективні моделі або переробляти робочі процеси для управління операційними витратами та забезпечення гарного досвіду користувача.
*   **Довіра, безпека та відповідність**: У багатьох застосуваннях важливо гарантувати, що агенти поводяться безпечно та етично. Спостережуваність забезпечує слід дій та рішень агента. Це може бути використано для виявлення та усунення проблем, таких як ін'єкція запитів, генерація шкідливого контенту або неправильне поводження з персональною інформацією (PII). Наприклад, ви можете переглянути трейси, щоб зрозуміти, чому агент надав певну відповідь або використав конкретний інструмент.
*   **Цикли постійного вдосконалення**: Дані спостережуваності є основою ітеративного процесу розробки. Спостерігаючи, як агенти працюють у реальному світі, команди можуть визначати області для покращення, збирати дані для тонкого налаштування моделей та перевіряти вплив змін. Це створює цикл зворотного зв'язку, де виробничі інсайти з онлайн-оцінки інформують офлайн-експерименти та вдосконалення, що призводить до поступового покращення продуктивності агентів.

## Основні метрики для відстеження

Для моніторингу та розуміння поведінки агентів слід відстежувати низку метрик та сигналів. Хоча конкретні метрики можуть варіюватися залежно від призначення агента, деякі є універсально важливими.

Ось деякі з найпоширеніших метрик, які відстежують інструменти спостережуваності:

**Затримка:** Як швидко агент відповідає? Тривале очікування негативно впливає на досвід користувача. Ви повинні вимірювати затримку для завдань та окремих кроків, відстежуючи виконання агентів. Наприклад, агент, який витрачає 20 секунд на всі виклики моделі, може бути прискорений за допомогою швидшої моделі або паралельного виконання викликів моделі.

**Витрати:** Яка вартість одного виконання агента? AI-агенти покладаються на виклики LLM, які оплачуються за токен, або зовнішні API. Часте використання інструментів або кілька запитів можуть швидко збільшити витрати. Наприклад, якщо агент викликає LLM п'ять разів для незначного покращення якості, потрібно оцінити, чи виправдані витрати, або чи можна зменшити кількість викликів чи використовувати дешевшу модель. Моніторинг у реальному часі також допомагає виявляти несподівані сплески (наприклад, помилки, що спричиняють надмірні цикли API).

**Помилки запитів:** Скільки запитів агент не зміг виконати? Це може включати помилки API або невдалі виклики інструментів. Щоб зробити вашого агента більш стійким до таких ситуацій у виробництві, можна налаштувати резервні варіанти або повторні спроби. Наприклад, якщо постачальник LLM A недоступний, ви перемикаєтеся на постачальника LLM B як резервний варіант.

**Відгуки користувачів:** Впровадження прямої оцінки користувачів надає цінну інформацію. Це може включати явні рейтинги (👍палець вгору/👎вниз, ⭐1-5 зірок) або текстові коментарі. Постійно негативні відгуки повинні вас насторожити, оскільки це ознака того, що агент працює не так, як очікується.

**Неявні відгуки користувачів:** Поведінка користувачів надає непрямі відгуки навіть без явних оцінок. Це може включати негайне перефразування запитання, повторні запити або натискання кнопки повтору. Наприклад, якщо ви бачите, що користувачі повторно задають одне й те саме запитання, це ознака того, що агент працює не так, як очікується.

**Точність:** Як часто агент видає правильні або бажані результати? Визначення точності варіюються (наприклад, правильність вирішення задач, точність отримання інформації, задоволеність користувачів). Першим кроком є визначення того, що означає успіх для вашого агента. Ви можете відстежувати точність за допомогою автоматичних перевірок, оцінювальних балів або міток завершення завдань. Наприклад, позначення трейсів як "успішних" або "невдалих".

**Автоматизовані метрики оцінки:** Ви також можете налаштувати автоматизовані оцінки. Наприклад, можна використовувати LLM для оцінки результатів агента, наприклад, чи вони корисні, точні чи ні. Існує також кілька бібліотек з відкритим кодом, які допомагають оцінювати різні аспекти агента. Наприклад, [RAGAS](https://docs.ragas.io/) для агентів RAG або [LLM Guard](https://llm-guard.com/) для виявлення шкідливого контенту або ін'єкції запитів.

На практиці комбінація цих метрик забезпечує найкраще охоплення стану здоров'я AI-агента. У [прикладному ноутбуці](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) цього розділу ми покажемо, як ці метрики виглядають на реальних прикладах, але спочатку ми навчимося, як виглядає типовий робочий процес оцінки.

## Інструментування вашого агента

Щоб зібрати дані трейсів, вам потрібно інструментувати ваш код. Мета полягає в тому, щоб налаштувати код агента на створення трейсів і метрик, які можуть бути захоплені, оброблені та візуалізовані платформою спостережуваності.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) став галузевим стандартом для спостережуваності LLM. Він надає набір API, SDK та інструментів для генерації, збору та експорту телеметричних даних.

Існує багато бібліотек інструментування, які обгортають існуючі фреймворки агентів і дозволяють легко експортувати спани OpenTelemetry до інструменту спостережуваності. Нижче наведено приклад інструментування агента AutoGen за допомогою бібліотеки інструментування [OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

[Прикладний ноутбук](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) у цьому розділі продемонструє, як інструментувати вашого агента AutoGen.

**Ручне створення спанів:** Хоча бібліотеки інструментування забезпечують гарну базу, часто виникають випадки, коли потрібна більш детальна або спеціальна інформація. Ви можете вручну створювати спани, щоб додати спеціальну логіку застосування. Більш важливо, вони можуть збагачувати автоматично або вручну створені спани спеціальними атрибутами (також відомими як теги або метадані). Ці атрибути можуть включати бізнес-специфічні дані, проміжні обчислення або будь-який контекст, який може бути корисним для відлагодження або аналізу, наприклад `user_id`, `session_id` або `model_version`.

Приклад створення трейсів і спанів вручну за допомогою [Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Оцінка агентів

Спостережуваність надає нам метрики, але оцінка — це процес аналізу цих даних (та проведення тестів), щоб визначити, наскільки добре працює AI-агент і як його можна покращити. Іншими словами, як тільки у вас є трейси та метрики, як їх використовувати для оцінки агента та прийняття рішень?

Регулярна оцінка важлива, оскільки AI-агенти часто є недетермінованими і можуть змінюватися (через оновлення або зміни поведінки моделі) – без оцінки ви не дізнаєтеся, чи ваш "розумний агент" дійсно добре виконує свою роботу або чи він погіршився.

Існує дві категорії оцінки AI-агентів: **онлайн-оцінка** та **офлайн-оцінка**. Обидві є цінними і доповнюють одна одну. Зазвичай ми починаємо з офлайн-оцінки, оскільки це мінімально необхідний крок перед впровадженням будь-якого агента.

### Офлайн-оцінка

![Елементи набору даних у Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Це передбачає оцінку агента в контрольованому середовищі, зазвичай використовуючи тестові набори даних, а не запити реальних користувачів. Ви використовуєте підготовлені набори даних, де ви знаєте очікуваний результат або правильну поведінку, а потім запускаєте вашого агента на них.

Наприклад, якщо ви створили агента для вирішення математичних задач, у вас може бути [тестовий набір даних](https://huggingface.co/datasets/gsm8k) з 100 задачами з відомими відповідями. Офлайн-оцінка часто проводиться під час розробки (і може бути частиною CI/CD-пайплайнів), щоб перевірити покращення або запобігти регресіям. Перевага полягає в тому, що це **повторювано, і ви можете отримати чіткі метрики точності, оскільки у вас є істина**. Ви також можете симулювати запити користувачів і вимірювати відповіді агента щодо ідеальних відповідей або використовувати автоматизовані метрики, як описано вище.

Основна проблема офлайн-оцінки — забезпечення того, щоб ваш тестовий набір даних був всеосяжним і залишався актуальним – агент може добре працювати на фіксованому тестовому наборі, але стикатися з дуже різними запитами у виробництві. Тому слід регулярно оновлювати тестові набори новими крайніми випадками та прикладами, які відображають реальні сценарії​. Корисно мати суміш невеликих "швидких тестів" і більших наборів для оцінки: невеликі набори для швидких перевірок і більші для ширших метрик продуктивності​.

### Онлайн-оцінка

![Огляд метрик спостережуваності](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Це передбачає оцінку агента в реальному, реальному середовищі, тобто під час фактичного використання у виробництві. Онлайн-оцінка включає моніторинг продуктивності агента на реальних взаємодіях з користувачами та постійний аналіз результатів.

Наприклад, ви можете відстежувати рівень успіху, оцінки задоволеності користувачів або інші метрики на реальному трафіку. Перевага онлайн-оцінки полягає в тому, що вона **захоплює те, що ви могли не передбачити в лабораторних умовах** – ви можете спостерігати дрейф моделі з часом (якщо ефективність агента знижується через зміну шаблонів введення) і виявляти несподівані запити або ситуації, які не були у вашому тестовому наборі​. Це дає справжню картину того, як агент поводиться в реальному світі.

Онлайн-оцінка часто включає збір неявних і явних відгуків користувачів, як обговорювалося, а також, можливо, проведення тіньових тестів або A/B-тестів (де нова версія агента працює паралельно для порівняння зі старою). Проблема полягає в тому, що може бути складно отримати надійні мітки або оцінки для реальних взаємодій – ви можете покладатися на відгуки користувачів або метрики нижчого рівня (наприклад, чи натиснув користувач на результат).

### Поєднання двох підходів

Онлайн- та офлайн-оцінки не є взаємовиключними; вони дуже добре доповнюють одна одну. Інсайти з онлайн-моніторингу (наприклад, нові типи запитів користувачів, де агент працює погано) можуть бути використані для доповнення та покращення офлайн-тестових наборів даних. Навпа

- Уточніть визначені параметри, підказки та назви інструментів.  |
| Багатоагентна система працює нестабільно | - Уточніть підказки, надані кожному агенту, щоб вони були конкретними та відрізнялися одна від одної.<br>- Побудуйте ієрархічну систему, використовуючи "маршрутизуючого" або контролюючого агента, щоб визначити, який агент є правильним. |

Багато з цих проблем можна ефективніше виявити за наявності спостережуваності. Траси та метрики, які ми обговорювали раніше, допомагають точно визначити, на якому етапі робочого процесу агента виникають проблеми, що значно спрощує налагодження та оптимізацію.

## Управління витратами

Ось кілька стратегій для управління витратами на впровадження AI-агентів у виробництво:

**Використання менших моделей:** Малі мовні моделі (SLM) можуть добре виконувати певні агентські завдання і значно знижують витрати. Як згадувалося раніше, створення системи оцінки для визначення та порівняння продуктивності з більшими моделями — найкращий спосіб зрозуміти, наскільки добре SLM впорається з вашим завданням. Розгляньте можливість використання SLM для простих завдань, таких як класифікація намірів або вилучення параметрів, залишаючи більші моделі для складних міркувань.

**Використання маршрутизуючої моделі:** Схожа стратегія полягає у використанні різноманітних моделей різних розмірів. Ви можете використовувати LLM/SLM або безсерверну функцію для маршрутизації запитів залежно від їх складності до найбільш відповідних моделей. Це також допоможе знизити витрати, забезпечуючи при цьому продуктивність для відповідних завдань. Наприклад, маршрутизуйте прості запити до менших, швидших моделей, а дорогі великі моделі використовуйте лише для складних завдань.

**Кешування відповідей:** Визначення поширених запитів і завдань та надання відповідей до того, як вони проходять через вашу агентську систему, є хорошим способом зменшити обсяг схожих запитів. Ви навіть можете реалізувати процес для визначення, наскільки схожий запит на ваші кешовані запити, використовуючи більш базові AI-моделі. Ця стратегія може значно знизити витрати для часто задаваних питань або типових робочих процесів.

## Давайте подивимося, як це працює на практиці

У [прикладному ноутбуці цього розділу](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) ми побачимо приклади того, як можна використовувати інструменти спостережуваності для моніторингу та оцінки нашого агента.

## Попередній урок

[Шаблон проєктування метакогніції](../09-metacognition/README.md)

## Наступний урок

[MCP](../11-mcp/README.md)

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, зверніть увагу, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ мовою оригіналу слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується професійний людський переклад. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.