<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-24T07:54:44+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "ja"
}
-->
# 本番環境におけるAIエージェント: 可観測性と評価

AIエージェントが実験的なプロトタイプから実際のアプリケーションへと移行する中で、その挙動を理解し、パフォーマンスを監視し、出力を体系的に評価する能力が重要になります。

## 学習目標

このレッスンを完了すると、以下のことが理解できるようになります:
- エージェントの可観測性と評価の基本概念
- エージェントのパフォーマンス、コスト、効果を向上させる技術
- AIエージェントを体系的に評価する方法とその内容
- AIエージェントを本番環境にデプロイする際のコスト管理方法
- AutoGenを使用して構築したエージェントを計測する方法

このレッスンの目的は、「ブラックボックス」のエージェントを透明性があり、管理可能で信頼できるシステムに変えるための知識を提供することです。

_**注:** 安全で信頼できるAIエージェントをデプロイすることが重要です。[信頼できるAIエージェントの構築](./06-building-trustworthy-agents/README.md)のレッスンもぜひご覧ください。_

## トレースとスパン

[Langfuse](https://langfuse.com/)や[Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry)のような可観測性ツールは、通常、エージェントの実行をトレースとスパンとして表現します。

- **トレース**: ユーザーのクエリ処理のように、エージェントのタスク全体を開始から終了まで表します。
- **スパン**: トレース内の個々のステップを表します（例: 言語モデルの呼び出しやデータの取得）。

![Langfuseのトレースツリー](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

可観測性がない場合、AIエージェントは「ブラックボックス」のように感じられます。内部状態や推論が不透明で、問題の診断やパフォーマンスの最適化が困難です。一方、可観測性があると、エージェントは「ガラスボックス」となり、透明性が確保され、信頼性を構築し、意図通りに動作することを保証するために不可欠です。

## 本番環境での可観測性の重要性

AIエージェントを本番環境に移行する際には、新たな課題や要件が生じます。可観測性はもはや「あると便利」なものではなく、重要な能力となります。

* **デバッグと根本原因の分析**: エージェントが失敗したり予期しない出力を生成した場合、可観測性ツールはエラーの原因を特定するためのトレースを提供します。特に、複数のLLM呼び出し、ツールの相互作用、条件付きロジックを含む複雑なエージェントでは重要です。
* **レイテンシーとコスト管理**: AIエージェントは、トークンや呼び出しごとに課金されるLLMや外部APIに依存することが多いです。可観測性により、これらの呼び出しを正確に追跡し、遅すぎるまたは高コストな操作を特定できます。これにより、プロンプトの最適化、より効率的なモデルの選択、ワークフローの再設計が可能になり、運用コストを管理し、良好なユーザー体験を確保できます。
* **信頼性、安全性、コンプライアンス**: 多くのアプリケーションでは、エージェントが安全かつ倫理的に動作することを保証する必要があります。可観測性は、エージェントの行動や意思決定の監査証跡を提供します。これにより、プロンプトインジェクション、有害なコンテンツの生成、個人情報（PII）の誤処理などの問題を検出して軽減できます。例えば、トレースをレビューして、エージェントが特定の応答を提供した理由や特定のツールを使用した理由を理解できます。
* **継続的な改善ループ**: 可観測性データは、反復的な開発プロセスの基盤です。エージェントが実際の環境でどのように動作しているかを監視することで、改善点を特定し、モデルの微調整のためのデータを収集し、変更の影響を検証できます。これにより、オンライン評価から得られた本番環境の洞察がオフラインの実験や改良に反映され、エージェントのパフォーマンスが徐々に向上します。

## 追跡すべき主要な指標

エージェントの挙動を監視し理解するためには、さまざまな指標やシグナルを追跡する必要があります。エージェントの目的によって具体的な指標は異なる場合がありますが、いくつかは普遍的に重要です。

以下は、可観測性ツールが監視する最も一般的な指標の一部です:

**レイテンシー:** エージェントはどれくらい速く応答しますか？長い待ち時間はユーザー体験に悪影響を与えます。タスクや個々のステップのレイテンシーをトレースして測定する必要があります。例えば、すべてのモデル呼び出しに20秒かかるエージェントは、より高速なモデルを使用するか、モデル呼び出しを並列化することで加速できます。

**コスト:** エージェント実行ごとの費用はどれくらいですか？AIエージェントは、トークンごとに課金されるLLM呼び出しや外部APIに依存します。頻繁なツール使用や複数のプロンプトは、コストを急速に増加させる可能性があります。例えば、エージェントが品質をわずかに向上させるためにLLMを5回呼び出す場合、そのコストが正当化されるか、呼び出し回数を減らすか、より安価なモデルを使用するべきかを評価する必要があります。リアルタイムの監視は、予期しないスパイク（例: 過剰なAPIループを引き起こすバグ）を特定するのにも役立ちます。

**リクエストエラー:** エージェントが失敗したリクエストの数はどれくらいですか？これには、APIエラーやツール呼び出しの失敗が含まれます。本番環境でこれらに対してエージェントをより堅牢にするために、フォールバックやリトライを設定できます。例えば、LLMプロバイダーAがダウンしている場合、バックアップとしてLLMプロバイダーBに切り替えることができます。

**ユーザーフィードバック:** ユーザーからの直接的な評価は貴重な洞察を提供します。これには、明示的な評価（👍/👎、⭐1-5など）やテキストコメントが含まれます。一貫して否定的なフィードバックがある場合、それはエージェントが期待通りに動作していない兆候です。

**暗黙的なユーザーフィードバック:** ユーザーの行動は、明示的な評価がなくても間接的なフィードバックを提供します。これには、即座の質問の言い換え、繰り返しのクエリ、リトライボタンのクリックなどが含まれます。例えば、ユーザーが同じ質問を繰り返し尋ねる場合、それはエージェントが期待通りに動作していない兆候です。

**正確性:** エージェントが正しいまたは望ましい出力を生成する頻度はどれくらいですか？正確性の定義はさまざまです（例: 問題解決の正確性、情報検索の正確性、ユーザー満足度）。最初のステップは、エージェントにとって成功が何を意味するかを定義することです。正確性は、自動チェック、評価スコア、タスク完了ラベルを通じて追跡できます。例えば、トレースを「成功」または「失敗」としてマークすることができます。

**自動評価指標:** 自動評価を設定することも可能です。例えば、エージェントの出力が役立つか、正確か、またはそうでないかをスコアリングするためにLLMを使用できます。また、エージェントのさまざまな側面をスコアリングするのに役立つオープンソースライブラリもいくつかあります。例: RAGエージェント用の[RAGAS](https://docs.ragas.io/)や、有害な言語やプロンプトインジェクションを検出するための[LLM Guard](https://llm-guard.com/)。

実際には、これらの指標を組み合わせることで、AIエージェントの健全性を最も包括的に把握できます。この章の[サンプルノートブック](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb)では、これらの指標が実際の例でどのように見えるかを示しますが、その前に、典型的な評価ワークフローについて学びます。

## エージェントの計測

トレースデータを収集するには、コードを計測する必要があります。目標は、エージェントコードを計測して、可観測性プラットフォームでキャプチャ、処理、可視化できるトレースや指標を生成することです。

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/)は、LLMの可観測性における業界標準として浮上しています。これは、テレメトリデータを生成、収集、エクスポートするためのAPI、SDK、ツールのセットを提供します。

既存のエージェントフレームワークをラップし、OpenTelemetryスパンを可観測性ツールに簡単にエクスポートできる計測ライブラリが多数あります。以下は、[OpenLit計測ライブラリ](https://github.com/openlit/openlit)を使用してAutoGenエージェントを計測する例です:

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

この章の[サンプルノートブック](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb)では、AutoGenエージェントを計測する方法を示します。

**手動スパン作成:** 計測ライブラリは良いベースラインを提供しますが、より詳細な情報やカスタム情報が必要な場合もあります。手動でスパンを作成して、カスタムアプリケーションロジックを追加することができます。さらに重要なのは、自動または手動で作成されたスパンにカスタム属性（タグやメタデータとも呼ばれる）を追加して強化できることです。これらの属性には、ビジネス固有のデータ、中間計算、デバッグや分析に役立つコンテキスト（例: `user_id`、`session_id`、`model_version`）が含まれます。

[Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3)を使用してトレースとスパンを手動で作成する例:

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## エージェントの評価

可観測性は指標を提供しますが、評価はそのデータを分析し（およびテストを実施し）、AIエージェントがどれだけうまく機能しているか、どのように改善できるかを判断するプロセスです。言い換えれば、トレースや指標を取得した後、それらを使用してエージェントを評価し、意思決定を行う方法です。

定期的な評価は重要です。なぜなら、AIエージェントは非決定論的であり、更新やモデルの挙動の変化を通じて進化する可能性があるからです。評価がなければ、「スマートエージェント」が実際にうまく機能しているのか、それとも性能が低下しているのかを知ることはできません。

AIエージェントの評価には、**オンライン評価**と**オフライン評価**の2つのカテゴリがあります。どちらも価値があり、相互に補完的です。通常、最低限必要なステップとしてオフライン評価から始めます。

### オフライン評価

![Langfuseのデータセット項目](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

これは、制御された環境でエージェントを評価するもので、通常はテストデータセットを使用し、ライブユーザークエリは使用しません。期待される出力や正しい挙動が分かっているキュレーションされたデータセットを使用し、それに対してエージェントを実行します。

例えば、数学の文章題を解くエージェントを構築した場合、既知の答えを持つ100問の[テストデータセット](https://huggingface.co/datasets/gsm8k)を使用するかもしれません。オフライン評価は、開発中に行われることが多く（CI/CDパイプラインの一部になることもあります）、改善を確認したり、性能低下を防ぐために使用されます。その利点は、**再現性があり、正解があるため明確な正確性指標を得られる**ことです。また、ユーザークエリをシミュレートし、エージェントの応答を理想的な答えと比較したり、前述の自動指標を使用して測定することもできます。

オフライン評価の主な課題は、テストデータセットが包括的であり続けること、そして現実に即したものであることを保証することです。エージェントが固定されたテストセットでうまく機能しても、本番環境では非常に異なるクエリに直面する可能性があります。そのため、テストセットを新しいエッジケースや現実のシナリオを反映した例で更新し続ける必要があります。小規模な「スモークテスト」ケースと大規模な評価セットを組み合わせると効果的です。小規模なセットは迅速なチェックに、大規模なセットは広範な性能指標に役立ちます。

### オンライン評価

![可観測性指標の概要](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

これは、ライブの実際の環境、つまり本番環境での実際の使用中にエージェントを評価することを指します。オンライン評価では、エージェントのパフォーマンスを実際のユーザーインタラクションで継続的に監視し、結果を分析します。

例えば、成功率、ユーザー満足度スコア、またはライブトラフィック上の他の指標を追跡するかもしれません。オンライン評価の利点は、**ラボ環境では予測できないことを捉えられる**ことです。モデルのドリフト（入力パターンの変化に伴うエージェントの効果の低下）を観察したり、テストデータには含まれていなかった予期しないクエリや状況をキャッチできます。本番環境でのエージェントの実際の挙動を正確に把握できます。

オンライン評価では、前述のように暗黙的および明示的なユーザーフィードバックを収集したり、シャドウテストやA/Bテスト（新しいバージョンのエージェントを旧バージョンと並行して実行して比較）を実施することもあります。課題としては、ライブインタラクションの信頼性の高いラベルやスコアを得るのが難しい場合があることです。ユーザーフィードバックや下流の指標（例: ユーザーが結果をクリックしたかどうか）に依存することが多いです。

### 両者の組み合わせ

オンライン評価とオフライン評価は相互排他的ではなく、非常に補完的です。オンライン監視から得られた洞察（例: エージェントがうまく機能しない新しいタイプのユーザークエリ）は、オフラインテストデータセットを

- 定義されたパラメータ、プロンプト、ツールの名前を洗練させる。  
| マルチエージェントシステムが一貫して動作しない | - 各エージェントに与えるプロンプトを見直し、それぞれが具体的で明確に区別されるようにする。<br>- "ルーティング"やコントローラーエージェントを使用して、どのエージェントが適切かを判断する階層的なシステムを構築する。 |

これらの問題の多くは、可観測性を導入することでより効果的に特定できます。前述したトレースやメトリクスは、エージェントのワークフローのどこで問題が発生しているかを正確に特定するのに役立ち、デバッグや最適化を大幅に効率化します。

## コスト管理

AIエージェントを本番環境にデプロイする際のコストを管理するための戦略を以下に示します：

**小型モデルの使用:** Small Language Models (SLMs) は、特定のエージェントユースケースで十分な性能を発揮し、コストを大幅に削減できます。前述のように、評価システムを構築して大規模モデルとの性能を比較することで、SLMがユースケースにどれだけ適しているかを理解するのが最善です。意図の分類やパラメータ抽出のような単純なタスクにはSLMを使用し、複雑な推論には大規模モデルを使用することを検討してください。

**ルーターモデルの使用:** 似たような戦略として、さまざまなモデルやサイズを組み合わせて使用する方法があります。LLM/SLMやサーバーレス関数を使用して、リクエストの複雑さに基づいて最適なモデルにルーティングすることができます。これにより、コストを削減しつつ、適切なタスクでの性能を確保できます。例えば、単純なクエリは小型で高速なモデルにルーティングし、複雑な推論タスクには高価な大規模モデルを使用する、といった方法です。

**レスポンスのキャッシュ:** 共通のリクエストやタスクを特定し、それらのレスポンスをエージェントシステムを通す前に提供することで、類似リクエストの量を削減できます。さらに、より基本的なAIモデルを使用して、リクエストがキャッシュされたリクエストとどれだけ類似しているかを特定するフローを実装することも可能です。この戦略は、よくある質問や一般的なワークフローに対してコストを大幅に削減します。

## 実際にどのように機能するか見てみましょう

このセクションの[サンプルノートブック](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb)では、可観測性ツールを使用してエージェントを監視・評価する方法の例を確認できます。

## 前のレッスン

[メタ認知デザインパターン](../09-metacognition/README.md)

## 次のレッスン

[MCP](../11-mcp/README.md)

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤認について、当方は一切の責任を負いません。