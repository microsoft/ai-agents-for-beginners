<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cdfd0acc8592c1af14f8637833450375",
  "translation_date": "2025-08-30T07:47:20+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "ja"
}
-->
# AIエージェントの運用：観測性と評価

[![AIエージェントの運用](../../../translated_images/lesson-10-thumbnail.2b79a30773db093e0b4fb47aaa618069e0afb4745fad4836526cf51df87f9ac9.ja.png)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

AIエージェントが実験的なプロトタイプから実際のアプリケーションへと移行するにつれ、その挙動を理解し、パフォーマンスを監視し、出力を体系的に評価する能力が重要になります。

## 学習目標

このレッスンを完了すると、以下について理解できるようになります：
- エージェントの観測性と評価の基本概念
- エージェントのパフォーマンス、コスト、効果を向上させるための技術
- AIエージェントを体系的に評価する方法と評価すべき内容
- AIエージェントを運用環境に展開する際のコスト管理方法
- AutoGenを使用して構築されたエージェントの計測方法

このレッスンの目的は、「ブラックボックス」エージェントを透明性があり、管理可能で信頼性の高いシステムに変えるための知識を提供することです。

_**注:** 安全で信頼できるAIエージェントを展開することが重要です。[信頼できるAIエージェントの構築](./06-building-trustworthy-agents/README.md)のレッスンもぜひご覧ください。_

## トレースとスパン

[Langfuse](https://langfuse.com/)や[Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry)などの観測性ツールは、通常、エージェントの実行をトレースとスパンとして表現します。

- **トレース**: エージェントのタスク全体を開始から終了まで表します（例：ユーザーのクエリを処理する）。
- **スパン**: トレース内の個々のステップを表します（例：言語モデルの呼び出しやデータの取得）。

![Langfuseのトレースツリー](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

観測性がない場合、AIエージェントは「ブラックボックス」のように感じられます。内部状態や推論が不透明で、問題の診断やパフォーマンスの最適化が困難になります。一方、観測性がある場合、エージェントは「ガラスボックス」となり、透明性が確保されます。これは信頼を構築し、エージェントが意図通りに動作することを保証するために不可欠です。

## 運用環境で観測性が重要な理由

AIエージェントを運用環境に移行する際には、新たな課題や要件が発生します。観測性はもはや「あると便利」なものではなく、重要な能力となります：

* **デバッグと原因分析**: エージェントが失敗したり予期しない出力を生成した場合、観測性ツールはエラーの原因を特定するためのトレースを提供します。特に、複数のLLM呼び出し、ツールの相互作用、条件付きロジックを含む複雑なエージェントでは重要です。
* **遅延とコスト管理**: AIエージェントは、トークンや呼び出しごとに課金されるLLMや外部APIに依存することが多いです。観測性によりこれらの呼び出しを正確に追跡し、過度に遅いまたは高価な操作を特定できます。これにより、プロンプトの最適化、効率的なモデルの選択、ワークフローの再設計が可能となり、運用コストを管理し、良好なユーザー体験を確保できます。
* **信頼性、安全性、コンプライアンス**: 多くのアプリケーションでは、エージェントが安全かつ倫理的に動作することが重要です。観測性はエージェントの行動や決定の監査証跡を提供します。これにより、プロンプトインジェクション、有害なコンテンツの生成、個人識別情報（PII）の誤処理などの問題を検出して軽減できます。例えば、トレースをレビューすることで、エージェントが特定の応答を提供した理由や特定のツールを使用した理由を理解できます。
* **継続的改善ループ**: 観測性データは反復的な開発プロセスの基盤です。エージェントが実世界でどのように動作しているかを監視することで、改善点を特定し、モデルの微調整のためのデータを収集し、変更の影響を検証できます。これにより、オンライン評価から得られる運用インサイトがオフラインの実験と改良に情報を提供し、エージェントのパフォーマンスを段階的に向上させるフィードバックループが形成されます。

## 追跡すべき主要な指標

エージェントの挙動を監視し理解するためには、さまざまな指標やシグナルを追跡する必要があります。具体的な指標はエージェントの目的によって異なる場合がありますが、いくつかは普遍的に重要です。

以下は、観測性ツールが監視する一般的な指標の例です：

**遅延:** エージェントはどれくらい迅速に応答するか？長い待ち時間はユーザー体験に悪影響を与えます。エージェントの実行をトレースすることで、タスクや個々のステップの遅延を測定できます。例えば、すべてのモデル呼び出しに20秒かかるエージェントは、より高速なモデルを使用するか、モデル呼び出しを並列で実行することで加速できます。

**コスト:** エージェントの実行ごとの費用はどれくらいか？AIエージェントは、トークンごとに課金されるLLM呼び出しや外部APIに依存します。頻繁なツール使用や複数のプロンプトはコストを急速に増加させる可能性があります。例えば、エージェントが品質をわずかに向上させるためにLLMを5回呼び出す場合、そのコストが正当化されるか、呼び出し回数を減らすか、より安価なモデルを使用する必要があります。リアルタイムの監視は、予期しないスパイク（例：バグによる過剰なAPIループ）を特定するのにも役立ちます。

**リクエストエラー:** エージェントが失敗したリクエストの数はどれくらいか？これにはAPIエラーやツール呼び出しの失敗が含まれます。これらに対してエージェントを運用環境でより堅牢にするために、フォールバックやリトライを設定できます。例えば、LLMプロバイダーAがダウンしている場合、バックアップとしてLLMプロバイダーBに切り替えることができます。

**ユーザーフィードバック:** ユーザーによる直接評価は貴重な洞察を提供します。これには明示的な評価（👍良い/👎悪い、⭐1-5の星評価）やテキストコメントが含まれます。一貫して否定的なフィードバックがある場合、それはエージェントが期待通りに動作していない兆候です。

**暗黙的なユーザーフィードバック:** ユーザーの行動は、明示的な評価がなくても間接的なフィードバックを提供します。これには、質問の即時再構成、繰り返しのクエリ、またはリトライボタンのクリックが含まれます。例えば、ユーザーが同じ質問を繰り返し尋ねる場合、それはエージェントが期待通りに動作していない兆候です。

**正確性:** エージェントが正しいまたは望ましい出力を生成する頻度はどれくらいか？正確性の定義は異なります（例：問題解決の正確性、情報検索の正確性、ユーザー満足度）。まず、エージェントにとって成功が何を意味するかを定義する必要があります。正確性は自動チェック、評価スコア、タスク完了ラベルを通じて追跡できます。例えば、トレースを「成功」または「失敗」としてマークすることができます。

**自動評価指標:** 自動評価を設定することも可能です。例えば、エージェントの出力が役立つか、正確かどうかをLLMを使用してスコアリングすることができます。また、エージェントのさまざまな側面をスコアリングするためのオープンソースライブラリもいくつかあります。例：[RAGAS](https://docs.ragas.io/)はRAGエージェント向け、[LLM Guard](https://llm-guard.com/)は有害な言語やプロンプトインジェクションを検出するためのものです。

実際には、これらの指標を組み合わせることで、AIエージェントの健康状態を最も包括的に把握できます。この章の[例のノートブック](./code_samples/10_autogen_evaluation.ipynb)では、これらの指標が実際の例でどのように見えるかを示しますが、まず典型的な評価ワークフローについて学びます。

## エージェントの計測

トレースデータを収集するには、コードを計測する必要があります。目標は、エージェントコードを計測して、観測性プラットフォームでキャプチャ、処理、可視化できるトレースや指標を生成することです。

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/)はLLM観測性の業界標準として台頭しています。API、SDK、ツールのセットを提供し、テレメトリデータの生成、収集、エクスポートを可能にします。

既存のエージェントフレームワークをラップし、OpenTelemetryスパンを観測性ツールに簡単にエクスポートできる計測ライブラリが多数あります。以下は、[OpenLit計測ライブラリ](https://github.com/openlit/openlit)を使用してAutoGenエージェントを計測する例です：

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

この章の[例のノートブック](./code_samples/10_autogen_evaluation.ipynb)では、AutoGenエージェントを計測する方法を示します。

**手動スパン作成:** 計測ライブラリは良い基盤を提供しますが、より詳細な情報やカスタム情報が必要な場合もあります。手動でスパンを作成してカスタムアプリケーションロジックを追加することができます。さらに、自動または手動で作成されたスパンにカスタム属性（タグやメタデータとも呼ばれる）を追加して豊かにすることができます。これらの属性には、ビジネス固有のデータ、中間計算、デバッグや分析に役立つコンテキスト（例：`user_id`、`session_id`、`model_version`）が含まれます。

[Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3)を使用してトレースとスパンを手動で作成する例：

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## エージェント評価

観測性は指標を提供しますが、評価はそのデータを分析し（およびテストを実施し）、AIエージェントがどれだけうまく機能しているか、どのように改善できるかを判断するプロセスです。つまり、トレースや指標を取得した後、それらを使用してエージェントを評価し、意思決定を行う方法です。

定期的な評価は重要です。AIエージェントは非決定論的であり、更新やモデルの挙動の変化を通じて進化する可能性があるため、評価がなければ「スマートエージェント」が実際にうまく機能しているか、または退化しているかを知ることができません。

AIエージェントの評価には、**オンライン評価**と**オフライン評価**の2つのカテゴリーがあります。どちらも価値があり、互いに補完し合います。通常、最低限必要なステップとしてオフライン評価から始めます。

### オフライン評価

![Langfuseのデータセット項目](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

これは、制御された環境でエージェントを評価することを指します。通常、テストデータセットを使用し、ライブユーザーのクエリは使用しません。期待される出力や正しい挙動が分かっているデータセットを使用し、エージェントを実行します。

例えば、数学の文章問題エージェントを構築した場合、既知の答えを持つ100問の[テストデータセット](https://huggingface.co/datasets/gsm8k)を使用するかもしれません。オフライン評価は開発中に行われることが多く（CI/CDパイプラインの一部になることもあります）、改善を確認したり退化を防ぐために使用されます。その利点は、**再現可能であり、正解があるため明確な正確性指標を得られること**です。理想的な回答と比較してエージェントの応答を測定したり、前述の自動指標を使用することもできます。

オフライン評価の主な課題は、テストデータセットが包括的であり、関連性を維持することです。エージェントが固定されたテストセットで良好なパフォーマンスを示しても、運用環境では非常に異なるクエリに直面する可能性があります。そのため、テストセットを新しいエッジケースや実世界のシナリオを反映した例で更新し続ける必要があります。小規模な「スモークテスト」ケースと大規模な評価セットを組み合わせることが有用です：小規模セットは迅速なチェック用、大規模セットは広範なパフォーマンス指標用です。

### オンライン評価

![観測性指標の概要](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

これは、ライブの実世界環境、つまり実際の運用中にエージェントを評価することを指します。オンライン評価では、エージェントのパフォーマンスを実際のユーザーとのインタラクションで継続的に監視し、結果を分析します。

例えば、成功率、ユーザー満足度スコア、またはライブトラフィックの他の指標を追跡するかもしれません。オンライン評価の利点は、**ラボ環境では予期しないことを捉えることができる**点です。モデルのドリフト（入力パターンの変化に伴いエージェントの有効性が低下すること）を観察したり、テストデータには含まれていなかった予期しないクエリや状況を捉えることができます。これにより、エージェントが実際の環境でどのように動作するかの真の姿を把握できます。

オンライン評価では、前述のように暗黙的および明示的なユーザーフィードバックを収集したり、シャドウテストやA/Bテスト（新しいバージョンのエージェントを旧バージョンと並行して実行して比較する）を実施することがあります。課題としては、ライブインタラクションの信頼性の高いラベルやスコアを取得するのが難しい場合があることです。ユーザーフィードバックや下流の指標（例：ユーザーが結果をクリックしたかどうか）に依存することがあります。

### 両者の組み合わせ

オンライン評価とオフライン評価は相互排他的ではなく、非常に補完的です。オンライン監視から得られるインサイト（例：エージェントがうまく機能しない新しいタイプのユーザークエリ）は、オフラインのテストデータセットを拡張し改善するために使用できます。逆に、オフラインテストで良好なパフォーマンスを示したエージェントは、より自信を持って展開しオンラインで監視

- 複雑なタスクで推論や計画が必要な場合は、推論に特化した大規模モデルを使用することを検討してください。  
| AIエージェントツールの呼び出しがうまく機能しない場合 | - エージェントシステム外でツールの出力をテストし、検証する。<br>- 定義されたパラメータ、プロンプト、ツールの命名を改善する。 |
| マルチエージェントシステムが一貫して機能しない場合 | - 各エージェントに与えるプロンプトを具体的で互いに異なるものにするよう改善する。<br>- 「ルーティング」またはコントローラーエージェントを使用して、どのエージェントが適切かを判断する階層的なシステムを構築する。 |

これらの問題の多くは、観測性を導入することでより効果的に特定できます。前述したトレースやメトリクスは、エージェントのワークフローのどこで問題が発生しているかを正確に特定するのに役立ち、デバッグや最適化をより効率的に行うことができます。

## コスト管理

AIエージェントを本番環境に展開する際のコスト管理のための戦略を以下に示します：

**小型モデルの使用:** 小型言語モデル（SLM）は、特定のエージェント的ユースケースで十分な性能を発揮し、コストを大幅に削減できます。前述の通り、評価システムを構築して性能を大規模モデルと比較することで、SLMがユースケースにどれだけ適しているかを理解するのが最善です。意図の分類やパラメータ抽出のような簡単なタスクにはSLMを使用し、複雑な推論には大規模モデルを使用することを検討してください。

**ルーターモデルの使用:** 似たような戦略として、モデルの多様性とサイズを活用する方法があります。LLM/SLMやサーバーレス関数を使用して、リクエストの複雑さに基づいて最適なモデルにルーティングすることができます。これにより、コストを削減しつつ、適切なタスクでの性能を確保できます。例えば、簡単なクエリは小型で高速なモデルにルーティングし、複雑な推論タスクには高価な大規模モデルを使用する、といった方法です。

**レスポンスのキャッシュ:** 共通のリクエストやタスクを特定し、それらのレスポンスをエージェントシステムを通過する前に提供することで、類似したリクエストの量を減らすことができます。さらに、基本的なAIモデルを使用してリクエストがキャッシュされたものとどれだけ類似しているかを特定するフローを実装することも可能です。この戦略は、よくある質問や共通のワークフローに対してコストを大幅に削減するのに役立ちます。

## 実際にどのように機能するか見てみましょう

このセクションの[例のノートブック](./code_samples/10_autogen_evaluation.ipynb)では、観測性ツールを使用してエージェントを監視・評価する方法の例を確認できます。

### AIエージェントの本番環境に関する質問はありますか？

[Azure AI Foundry Discord](https://aka.ms/ai-agents/discord)に参加して、他の学習者と交流したり、オフィスアワーに参加したり、AIエージェントに関する質問に答えてもらいましょう。

## 前のレッスン

[メタ認知デザインパターン](../09-metacognition/README.md)

## 次のレッスン

[エージェントプロトコル](../11-agentic-protocols/README.md)

---

**免責事項**:  
この文書は、AI翻訳サービス [Co-op Translator](https://github.com/Azure/co-op-translator) を使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確な部分が含まれる可能性があることをご承知ください。元の言語で記載された文書が正式な情報源とみなされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤った解釈について、当方は一切の責任を負いません。