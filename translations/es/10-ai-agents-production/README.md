<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "cdfd0acc8592c1af14f8637833450375",
  "translation_date": "2025-08-30T07:09:42+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "es"
}
-->
# Agentes de IA en Producci√≥n: Observabilidad y Evaluaci√≥n

[![Agentes de IA en Producci√≥n](../../../translated_images/lesson-10-thumbnail.2b79a30773db093e0b4fb47aaa618069e0afb4745fad4836526cf51df87f9ac9.es.png)](https://youtu.be/l4TP6IyJxmQ?si=reGOyeqjxFevyDq9)

A medida que los agentes de IA pasan de ser prototipos experimentales a aplicaciones del mundo real, la capacidad de comprender su comportamiento, monitorear su rendimiento y evaluar sistem√°ticamente sus resultados se vuelve crucial.

## Objetivos de Aprendizaje

Despu√©s de completar esta lecci√≥n, sabr√°s c√≥mo/entender√°s:
- Conceptos fundamentales de observabilidad y evaluaci√≥n de agentes
- T√©cnicas para mejorar el rendimiento, los costos y la efectividad de los agentes
- Qu√© evaluar y c√≥mo hacerlo de manera sistem√°tica en tus agentes de IA
- C√≥mo controlar los costos al implementar agentes de IA en producci√≥n
- C√≥mo instrumentar agentes construidos con AutoGen

El objetivo es proporcionarte el conocimiento necesario para transformar tus agentes de "caja negra" en sistemas transparentes, manejables y confiables.

_**Nota:** Es importante implementar agentes de IA que sean seguros y confiables. Consulta la lecci√≥n [Construyendo Agentes de IA Confiables](./06-building-trustworthy-agents/README.md) tambi√©n._

## Trazas y Spans

Las herramientas de observabilidad como [Langfuse](https://langfuse.com/) o [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) suelen representar las ejecuciones de los agentes como trazas y spans.

- **Traza** representa una tarea completa del agente desde el inicio hasta el final (como manejar una consulta de usuario).
- **Spans** son pasos individuales dentro de la traza (como llamar a un modelo de lenguaje o recuperar datos).

![√Årbol de trazas en Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

Sin observabilidad, un agente de IA puede sentirse como una "caja negra": su estado interno y razonamiento son opacos, lo que dificulta diagnosticar problemas u optimizar el rendimiento. Con observabilidad, los agentes se convierten en "cajas de cristal", ofreciendo transparencia que es vital para generar confianza y garantizar que operen seg√∫n lo previsto.

## Por qu√© la Observabilidad es Importante en Entornos de Producci√≥n

La transici√≥n de los agentes de IA a entornos de producci√≥n introduce un nuevo conjunto de desaf√≠os y requisitos. La observabilidad deja de ser un "lujo" y se convierte en una capacidad cr√≠tica:

*   **Depuraci√≥n y An√°lisis de Causas Ra√≠z**: Cuando un agente falla o produce un resultado inesperado, las herramientas de observabilidad proporcionan las trazas necesarias para identificar la fuente del error. Esto es especialmente importante en agentes complejos que pueden involucrar m√∫ltiples llamadas a LLM, interacciones con herramientas y l√≥gica condicional.
*   **Gesti√≥n de Latencia y Costos**: Los agentes de IA suelen depender de LLMs y otras APIs externas que se facturan por token o por llamada. La observabilidad permite un seguimiento preciso de estas llamadas, ayudando a identificar operaciones que son excesivamente lentas o costosas. Esto permite a los equipos optimizar prompts, seleccionar modelos m√°s eficientes o redise√±ar flujos de trabajo para gestionar costos operativos y garantizar una buena experiencia de usuario.
*   **Confianza, Seguridad y Cumplimiento**: En muchas aplicaciones, es importante garantizar que los agentes se comporten de manera segura y √©tica. La observabilidad proporciona un registro de auditor√≠a de las acciones y decisiones del agente. Esto puede usarse para detectar y mitigar problemas como la inyecci√≥n de prompts, la generaci√≥n de contenido da√±ino o el manejo indebido de informaci√≥n personal identificable (PII). Por ejemplo, puedes revisar trazas para entender por qu√© un agente proporcion√≥ una cierta respuesta o utiliz√≥ una herramienta espec√≠fica.
*   **Bucles de Mejora Continua**: Los datos de observabilidad son la base de un proceso de desarrollo iterativo. Al monitorear c√≥mo los agentes se desempe√±an en el mundo real, los equipos pueden identificar √°reas de mejora, recopilar datos para ajustar modelos y validar el impacto de los cambios. Esto crea un bucle de retroalimentaci√≥n donde los conocimientos de producci√≥n obtenidos de la evaluaci√≥n en l√≠nea informan la experimentaci√≥n y refinamiento fuera de l√≠nea, llevando a un rendimiento progresivamente mejor de los agentes.

## M√©tricas Clave para Monitorear

Para monitorear y comprender el comportamiento de los agentes, se deben rastrear una variedad de m√©tricas y se√±ales. Aunque las m√©tricas espec√≠ficas pueden variar seg√∫n el prop√≥sito del agente, algunas son universalmente importantes.

Aqu√≠ est√°n algunas de las m√©tricas m√°s comunes que las herramientas de observabilidad monitorean:

**Latencia:** ¬øQu√© tan r√°pido responde el agente? Los tiempos de espera prolongados impactan negativamente la experiencia del usuario. Debes medir la latencia de las tareas y los pasos individuales trazando las ejecuciones del agente. Por ejemplo, un agente que tarda 20 segundos en todas las llamadas al modelo podr√≠a acelerarse utilizando un modelo m√°s r√°pido o ejecutando las llamadas al modelo en paralelo.

**Costos:** ¬øCu√°l es el costo por ejecuci√≥n del agente? Los agentes de IA dependen de llamadas a LLM que se facturan por token o APIs externas. El uso frecuente de herramientas o m√∫ltiples prompts puede aumentar r√°pidamente los costos. Por ejemplo, si un agente llama a un LLM cinco veces para una mejora marginal en la calidad, debes evaluar si el costo est√° justificado o si podr√≠as reducir el n√∫mero de llamadas o usar un modelo m√°s econ√≥mico. El monitoreo en tiempo real tambi√©n puede ayudar a identificar picos inesperados (por ejemplo, errores que causan bucles excesivos de API).

**Errores de Solicitud:** ¬øCu√°ntas solicitudes fallaron en el agente? Esto puede incluir errores de API o llamadas fallidas a herramientas. Para hacer que tu agente sea m√°s robusto en producci√≥n, puedes configurar alternativas o reintentos. Por ejemplo, si el proveedor de LLM A est√° ca√≠do, cambias al proveedor de LLM B como respaldo.

**Retroalimentaci√≥n del Usuario:** Implementar evaluaciones directas de los usuarios proporciona informaci√≥n valiosa. Esto puede incluir calificaciones expl√≠citas (üëçpulgar arriba/üëéabajo, ‚≠ê1-5 estrellas) o comentarios textuales. La retroalimentaci√≥n negativa constante deber√≠a alertarte, ya que es una se√±al de que el agente no est√° funcionando como se espera.

**Retroalimentaci√≥n Impl√≠cita del Usuario:** Los comportamientos de los usuarios proporcionan retroalimentaci√≥n indirecta incluso sin calificaciones expl√≠citas. Esto puede incluir reformulaci√≥n inmediata de preguntas, consultas repetidas o hacer clic en un bot√≥n de reintento. Por ejemplo, si ves que los usuarios preguntan repetidamente lo mismo, esto es una se√±al de que el agente no est√° funcionando como se espera.

**Precisi√≥n:** ¬øCon qu√© frecuencia el agente produce resultados correctos o deseables? Las definiciones de precisi√≥n var√≠an (por ejemplo, correcci√≥n en la resoluci√≥n de problemas, precisi√≥n en la recuperaci√≥n de informaci√≥n, satisfacci√≥n del usuario). El primer paso es definir c√≥mo se ve el √©xito para tu agente. Puedes rastrear la precisi√≥n mediante verificaciones automatizadas, puntuaciones de evaluaci√≥n o etiquetas de finalizaci√≥n de tareas. Por ejemplo, marcar trazas como "exitosas" o "fallidas".

**M√©tricas de Evaluaci√≥n Automatizada:** Tambi√©n puedes configurar evaluaciones automatizadas. Por ejemplo, puedes usar un LLM para calificar la salida del agente, evaluando si es √∫til, precisa o no. Tambi√©n hay varias bibliotecas de c√≥digo abierto que te ayudan a calificar diferentes aspectos del agente. Por ejemplo, [RAGAS](https://docs.ragas.io/) para agentes RAG o [LLM Guard](https://llm-guard.com/) para detectar lenguaje da√±ino o inyecci√≥n de prompts.

En la pr√°ctica, una combinaci√≥n de estas m√©tricas ofrece la mejor cobertura de la salud de un agente de IA. En el [notebook de ejemplo](./code_samples/10_autogen_evaluation.ipynb) de este cap√≠tulo, te mostraremos c√≥mo se ven estas m√©tricas en ejemplos reales, pero primero aprenderemos c√≥mo se ve un flujo t√≠pico de evaluaci√≥n.

## Instrumenta tu Agente

Para recopilar datos de trazas, necesitar√°s instrumentar tu c√≥digo. El objetivo es instrumentar el c√≥digo del agente para emitir trazas y m√©tricas que puedan ser capturadas, procesadas y visualizadas por una plataforma de observabilidad.

**OpenTelemetry (OTel):** [OpenTelemetry](https://opentelemetry.io/) se ha convertido en un est√°ndar de la industria para la observabilidad de LLM. Proporciona un conjunto de APIs, SDKs y herramientas para generar, recopilar y exportar datos de telemetr√≠a.

Existen muchas bibliotecas de instrumentaci√≥n que envuelven los marcos de trabajo de agentes existentes y facilitan la exportaci√≥n de spans de OpenTelemetry a una herramienta de observabilidad. A continuaci√≥n, se muestra un ejemplo de c√≥mo instrumentar un agente de AutoGen con la biblioteca de instrumentaci√≥n [OpenLit](https://github.com/openlit/openlit):

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

El [notebook de ejemplo](./code_samples/10_autogen_evaluation.ipynb) en este cap√≠tulo demostrar√° c√≥mo instrumentar tu agente AutoGen.

**Creaci√≥n Manual de Spans:** Aunque las bibliotecas de instrumentaci√≥n proporcionan una buena base, a menudo hay casos donde se necesita informaci√≥n m√°s detallada o personalizada. Puedes crear spans manualmente para agregar l√≥gica personalizada de la aplicaci√≥n. M√°s importante a√∫n, pueden enriquecer spans creados autom√°ticamente o manualmente con atributos personalizados (tambi√©n conocidos como etiquetas o metadatos). Estos atributos pueden incluir datos espec√≠ficos del negocio, c√°lculos intermedios o cualquier contexto que pueda ser √∫til para depuraci√≥n o an√°lisis, como `user_id`, `session_id` o `model_version`.

Ejemplo de creaci√≥n de trazas y spans manualmente con el [SDK de Python de Langfuse](https://langfuse.com/docs/sdk/python/sdk-v3):

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## Evaluaci√≥n de Agentes

La observabilidad nos da m√©tricas, pero la evaluaci√≥n es el proceso de analizar esos datos (y realizar pruebas) para determinar qu√© tan bien est√° funcionando un agente de IA y c√≥mo puede mejorarse. En otras palabras, una vez que tienes esas trazas y m√©tricas, ¬øc√≥mo las usas para juzgar al agente y tomar decisiones?

La evaluaci√≥n regular es importante porque los agentes de IA suelen ser no deterministas y pueden evolucionar (a trav√©s de actualizaciones o cambios en el comportamiento del modelo). Sin evaluaci√≥n, no sabr√≠as si tu "agente inteligente" realmente est√° haciendo bien su trabajo o si ha retrocedido.

Hay dos categor√≠as de evaluaciones para agentes de IA: **evaluaci√≥n en l√≠nea** y **evaluaci√≥n fuera de l√≠nea**. Ambas son valiosas y se complementan entre s√≠. Por lo general, comenzamos con la evaluaci√≥n fuera de l√≠nea, ya que este es el paso m√≠nimo necesario antes de implementar cualquier agente.

### Evaluaci√≥n Fuera de L√≠nea

![Elementos del dataset en Langfuse](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

Esto implica evaluar al agente en un entorno controlado, generalmente utilizando conjuntos de datos de prueba, no consultas de usuarios en vivo. Usas conjuntos de datos curados donde sabes cu√°l es el resultado esperado o el comportamiento correcto, y luego ejecutas tu agente en ellos.

Por ejemplo, si construiste un agente para resolver problemas matem√°ticos, podr√≠as tener un [conjunto de datos de prueba](https://huggingface.co/datasets/gsm8k) de 100 problemas con respuestas conocidas. La evaluaci√≥n fuera de l√≠nea se realiza a menudo durante el desarrollo (y puede formar parte de los pipelines de CI/CD) para verificar mejoras o proteger contra regresiones. El beneficio es que es **repetible y puedes obtener m√©tricas claras de precisi√≥n, ya que tienes la verdad absoluta**. Tambi√©n podr√≠as simular consultas de usuarios y medir las respuestas del agente frente a respuestas ideales o usar m√©tricas automatizadas como se describi√≥ anteriormente.

El desaf√≠o clave con la evaluaci√≥n fuera de l√≠nea es garantizar que tu conjunto de datos de prueba sea completo y se mantenga relevante: el agente podr√≠a desempe√±arse bien en un conjunto de prueba fijo pero enfrentar consultas muy diferentes en producci√≥n. Por lo tanto, debes mantener los conjuntos de prueba actualizados con nuevos casos l√≠mite y ejemplos que reflejen escenarios del mundo real. Una mezcla de peque√±os casos de "prueba de humo" y conjuntos de evaluaci√≥n m√°s grandes es √∫til: conjuntos peque√±os para verificaciones r√°pidas y conjuntos m√°s grandes para m√©tricas de rendimiento m√°s amplias.

### Evaluaci√≥n en L√≠nea

![Resumen de m√©tricas de observabilidad](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

Esto se refiere a evaluar al agente en un entorno en vivo, en el mundo real, es decir, durante el uso real en producci√≥n. La evaluaci√≥n en l√≠nea implica monitorear el rendimiento del agente en interacciones reales con usuarios y analizar los resultados de manera continua.

Por ejemplo, podr√≠as rastrear tasas de √©xito, puntuaciones de satisfacci√≥n del usuario u otras m√©tricas en tr√°fico en vivo. La ventaja de la evaluaci√≥n en l√≠nea es que **captura cosas que podr√≠as no anticipar en un entorno de laboratorio**: puedes observar el cambio del modelo con el tiempo (si la efectividad del agente se degrada a medida que cambian los patrones de entrada) y detectar consultas inesperadas o situaciones que no estaban en tus datos de prueba. Proporciona una imagen real de c√≥mo se comporta el agente en el mundo real.

La evaluaci√≥n en l√≠nea a menudo implica recopilar retroalimentaci√≥n impl√≠cita y expl√≠cita de los usuarios, como se discuti√≥, y posiblemente ejecutar pruebas en paralelo o pruebas A/B (donde una nueva versi√≥n del agente se ejecuta en paralelo para comparar con la antigua). El desaf√≠o es que puede ser complicado obtener etiquetas o puntuaciones confiables para interacciones en vivo: podr√≠as depender de la retroalimentaci√≥n de los usuarios o m√©tricas posteriores (como si el usuario hizo clic en el resultado).

### Combinando ambas

Las evaluaciones en l√≠nea y fuera de l√≠nea no son mutuamente excluyentes; se complementan altamente. Los conocimientos obtenidos del monitoreo en l√≠nea (por ejemplo, nuevos tipos de consultas de usuarios donde el agente tiene un rendimiento deficiente) pueden usarse para ampliar y mejorar los conjuntos de datos de prueba fuera de l√≠nea. Por otro lado, los agentes que se desempe√±an bien en pruebas fuera de l√≠nea pueden implementarse y monitorearse en l√≠nea con mayor confianza.

De hecho, muchos equipos adoptan un bucle:

_evaluar fuera de l√≠nea -> implementar -> monitorear en l√≠nea -> recopilar nuevos casos de falla -> agregar al conjunto de datos fuera de l√≠nea -> refinar agente -> repetir_.

## Problemas Comunes

A medida que implementas agentes de IA en producci√≥n, puedes encontrarte con varios desaf√≠os. Aqu√≠ hay algunos problemas comunes y sus posibles soluciones:

| **Problema**    | **Soluci√≥n Potencial**   |
| ------------- | ------------------ |
| El agente de IA no realiza tareas de manera consistente | - Refinar el prompt dado al agente de IA; ser claro en los objetivos.<br>- Identificar d√≥nde dividir las tareas en subtareas y manejarlas con m√∫ltiples agentes puede ayudar. |
| El agente de IA entra en bucles continuos  | - Aseg√∫rate de tener t√©rminos y condiciones claros de finalizaci√≥n para que el agente sepa cu√°ndo detener el proceso. |

## Evaluaci√≥n y Optimizaci√≥n

### Identificaci√≥n de Problemas Comunes

Aqu√≠ hay algunos problemas comunes que pueden surgir al trabajar con sistemas de agentes de IA, junto con estrategias para abordarlos:

| **Problema**                                   | **Soluci√≥n**                                                                                     |
|------------------------------------------------|--------------------------------------------------------------------------------------------------|
| El modelo no responde correctamente            | - Ajusta los datos de entrenamiento para incluir ejemplos m√°s relevantes.<br>- Revisa los prompts para asegurarte de que sean claros y espec√≠ficos. |
| El modelo tarda demasiado en responder         | - Usa modelos m√°s peque√±os para tareas simples.<br>- Optimiza el flujo de trabajo para reducir la latencia. |
| El modelo no maneja tareas complejas           | - Para tareas complejas que requieren razonamiento y planificaci√≥n, utiliza un modelo m√°s grande especializado en estas tareas. |
| Las herramientas del agente de IA no funcionan bien | - Prueba y valida la salida de las herramientas fuera del sistema del agente.<br>- Refina los par√°metros definidos, los prompts y los nombres de las herramientas. |
| El sistema multi-agente no es consistente      | - Ajusta los prompts dados a cada agente para asegurarte de que sean espec√≠ficos y distintos entre s√≠.<br>- Construye un sistema jer√°rquico utilizando un agente "enrutador" o controlador para determinar cu√°l agente es el adecuado. |

Muchos de estos problemas pueden identificarse de manera m√°s efectiva si se cuenta con herramientas de observabilidad. Las trazas y m√©tricas que discutimos anteriormente ayudan a identificar exactamente d√≥nde ocurren los problemas en el flujo de trabajo del agente, haciendo que la depuraci√≥n y la optimizaci√≥n sean mucho m√°s eficientes.

## Gesti√≥n de Costos

Aqu√≠ tienes algunas estrategias para gestionar los costos de implementar agentes de IA en producci√≥n:

**Uso de Modelos M√°s Peque√±os:** Los Modelos de Lenguaje Peque√±os (SLMs, por sus siglas en ingl√©s) pueden funcionar bien en ciertos casos de uso de agentes y reducir significativamente los costos. Como se mencion√≥ anteriormente, construir un sistema de evaluaci√≥n para determinar y comparar el rendimiento frente a modelos m√°s grandes es la mejor manera de entender qu√© tan bien un SLM se ajustar√° a tu caso de uso. Considera usar SLMs para tareas m√°s simples como clasificaci√≥n de intenciones o extracci√≥n de par√°metros, reservando los modelos m√°s grandes para razonamientos complejos.

**Uso de un Modelo Enrutador:** Una estrategia similar es usar una diversidad de modelos y tama√±os. Puedes usar un LLM/SLM o una funci√≥n sin servidor para enrutar solicitudes seg√∫n su complejidad hacia los modelos m√°s adecuados. Esto tambi√©n ayudar√° a reducir costos mientras se asegura el rendimiento en las tareas correctas. Por ejemplo, enruta consultas simples a modelos m√°s peque√±os y r√°pidos, y utiliza modelos grandes y costosos solo para tareas de razonamiento complejo.

**Cach√© de Respuestas:** Identificar solicitudes y tareas comunes y proporcionar las respuestas antes de que pasen por tu sistema de agentes es una buena manera de reducir el volumen de solicitudes similares. Incluso puedes implementar un flujo para identificar qu√© tan similar es una solicitud a tus solicitudes en cach√© utilizando modelos de IA m√°s b√°sicos. Esta estrategia puede reducir significativamente los costos para preguntas frecuentes o flujos de trabajo comunes.

## Veamos c√≥mo funciona esto en la pr√°ctica

En el [notebook de ejemplo de esta secci√≥n](./code_samples/10_autogen_evaluation.ipynb), veremos ejemplos de c√≥mo podemos usar herramientas de observabilidad para monitorear y evaluar nuestro agente.

### ¬øTienes M√°s Preguntas sobre Agentes de IA en Producci√≥n?

√önete al [Discord de Azure AI Foundry](https://aka.ms/ai-agents/discord) para conectarte con otros estudiantes, asistir a horas de oficina y resolver tus dudas sobre agentes de IA.

## Lecci√≥n Anterior

[Patr√≥n de Dise√±o de Metacognici√≥n](../09-metacognition/README.md)

## Pr√≥xima Lecci√≥n

[Protocolos Ag√©nticos](../11-agentic-protocols/README.md)

---

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que puedan surgir del uso de esta traducci√≥n.