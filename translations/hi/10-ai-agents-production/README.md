<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8164484c16b1ed3287ef9dae9fc437c1",
  "translation_date": "2025-07-24T08:00:01+00:00",
  "source_file": "10-ai-agents-production/README.md",
  "language_code": "hi"
}
-->
# उत्पादन में AI एजेंट्स: अवलोकन और मूल्यांकन

जैसे-जैसे AI एजेंट्स प्रयोगात्मक प्रोटोटाइप से वास्तविक दुनिया के अनुप्रयोगों में बदलते हैं, उनके व्यवहार को समझने, प्रदर्शन की निगरानी करने और उनके आउटपुट का व्यवस्थित रूप से मूल्यांकन करने की क्षमता महत्वपूर्ण हो जाती है।

## सीखने के लक्ष्य

इस पाठ को पूरा करने के बाद, आप जानेंगे/समझेंगे:
- एजेंट अवलोकन और मूल्यांकन के मुख्य सिद्धांत
- एजेंट्स के प्रदर्शन, लागत और प्रभावशीलता को सुधारने की तकनीकें
- अपने AI एजेंट्स का व्यवस्थित रूप से क्या और कैसे मूल्यांकन करें
- उत्पादन में AI एजेंट्स को तैनात करते समय लागत को कैसे नियंत्रित करें
- AutoGen के साथ बनाए गए एजेंट्स को कैसे इंस्ट्रूमेंट करें

उद्देश्य आपको "ब्लैक बॉक्स" एजेंट्स को पारदर्शी, प्रबंधनीय और भरोसेमंद सिस्टम में बदलने का ज्ञान प्रदान करना है।

_**नोट:** सुरक्षित और भरोसेमंद AI एजेंट्स को तैनात करना महत्वपूर्ण है। [भरोसेमंद AI एजेंट्स बनाना](./06-building-trustworthy-agents/README.md) पाठ को भी देखें।_

## ट्रेसेस और स्पैन्स

अवलोकन उपकरण जैसे [Langfuse](https://langfuse.com/) या [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-azure-ai-foundry) आमतौर पर एजेंट रन को ट्रेसेस और स्पैन्स के रूप में प्रस्तुत करते हैं।

- **ट्रेस** एक पूर्ण एजेंट कार्य को शुरुआत से अंत तक दर्शाता है (जैसे उपयोगकर्ता क्वेरी को संभालना)।
- **स्पैन्स** ट्रेस के भीतर व्यक्तिगत चरण होते हैं (जैसे भाषा मॉडल को कॉल करना या डेटा प्राप्त करना)।

![Langfuse में ट्रेस ट्री](https://langfuse.com/images/cookbook/example-autogen-evaluation/trace-tree.png)

अवलोकन के बिना, एक AI एजेंट "ब्लैक बॉक्स" जैसा महसूस हो सकता है - इसकी आंतरिक स्थिति और तर्क अस्पष्ट होते हैं, जिससे समस्याओं का निदान करना या प्रदर्शन को अनुकूलित करना कठिन हो जाता है। अवलोकन के साथ, एजेंट्स "ग्लास बॉक्स" बन जाते हैं, जो पारदर्शिता प्रदान करते हैं, जो विश्वास बनाने और यह सुनिश्चित करने के लिए महत्वपूर्ण है कि वे इच्छित रूप से कार्य करें।

## उत्पादन वातावरण में अवलोकन क्यों महत्वपूर्ण है

AI एजेंट्स को उत्पादन वातावरण में स्थानांतरित करना नई चुनौतियों और आवश्यकताओं को पेश करता है। अवलोकन अब "अच्छा-से-होने वाला" नहीं है, बल्कि एक महत्वपूर्ण क्षमता है:

*   **डिबगिंग और रूट-कॉज़ विश्लेषण**: जब कोई एजेंट विफल होता है या अप्रत्याशित आउटपुट उत्पन्न करता है, तो अवलोकन उपकरण त्रुटि के स्रोत को pinpoint करने के लिए आवश्यक ट्रेसेस प्रदान करते हैं। यह विशेष रूप से जटिल एजेंट्स में महत्वपूर्ण है, जिनमें कई LLM कॉल्स, टूल इंटरैक्शन और कंडीशनल लॉजिक शामिल हो सकते हैं।
*   **लेटेंसी और लागत प्रबंधन**: AI एजेंट्स अक्सर LLMs और अन्य बाहरी APIs पर निर्भर होते हैं, जो प्रति टोकन या प्रति कॉल बिल किए जाते हैं। अवलोकन इन कॉल्स को सटीक रूप से ट्रैक करने की अनुमति देता है, जिससे अत्यधिक धीमी या महंगी ऑपरेशन्स की पहचान होती है। यह टीमों को प्रॉम्प्ट्स को अनुकूलित करने, अधिक कुशल मॉडल चुनने, या वर्कफ़्लो को फिर से डिज़ाइन करने में सक्षम बनाता है ताकि परिचालन लागत को प्रबंधित किया जा सके और उपयोगकर्ता अनुभव सुनिश्चित किया जा सके।
*   **विश्वास, सुरक्षा और अनुपालन**: कई अनुप्रयोगों में, यह सुनिश्चित करना महत्वपूर्ण है कि एजेंट सुरक्षित और नैतिक रूप से व्यवहार करें। अवलोकन एजेंट क्रियाओं और निर्णयों का ऑडिट ट्रेल प्रदान करता है। इसका उपयोग प्रॉम्प्ट इंजेक्शन, हानिकारक सामग्री उत्पन्न करने, या व्यक्तिगत पहचान योग्य जानकारी (PII) के गलत संचालन जैसे मुद्दों का पता लगाने और उन्हें कम करने के लिए किया जा सकता है। उदाहरण के लिए, आप ट्रेसेस की समीक्षा कर सकते हैं कि एजेंट ने एक निश्चित प्रतिक्रिया क्यों दी या एक विशिष्ट टूल का उपयोग क्यों किया।
*   **निरंतर सुधार चक्र**: अवलोकन डेटा एक पुनरावृत्त विकास प्रक्रिया की नींव है। यह देखकर कि एजेंट वास्तविक दुनिया में कैसे प्रदर्शन करते हैं, टीमें सुधार के क्षेत्रों की पहचान कर सकती हैं, मॉडल को फाइन-ट्यून करने के लिए डेटा एकत्र कर सकती हैं, और परिवर्तनों के प्रभाव को मान्य कर सकती हैं। यह एक फीडबैक लूप बनाता है जहां ऑनलाइन मूल्यांकन से उत्पादन अंतर्दृष्टि ऑफ़लाइन प्रयोग और परिष्करण को सूचित करती है, जिससे एजेंट प्रदर्शन में प्रगतिशील सुधार होता है।

## ट्रैक करने के लिए प्रमुख मेट्रिक्स

एजेंट व्यवहार की निगरानी और समझने के लिए, कई मेट्रिक्स और संकेतों को ट्रैक किया जाना चाहिए। जबकि एजेंट के उद्देश्य के आधार पर विशिष्ट मेट्रिक्स भिन्न हो सकते हैं, कुछ सार्वभौमिक रूप से महत्वपूर्ण हैं।

यहां कुछ सबसे सामान्य मेट्रिक्स हैं जिन्हें अवलोकन उपकरण मॉनिटर करते हैं:

**लेटेंसी:** एजेंट कितनी जल्दी प्रतिक्रिया देता है? लंबे इंतजार का समय उपयोगकर्ता अनुभव को नकारात्मक रूप से प्रभावित करता है। आपको एजेंट रन को ट्रेस करके कार्यों और व्यक्तिगत चरणों के लिए लेटेंसी को मापना चाहिए। उदाहरण के लिए, एक एजेंट जो सभी मॉडल कॉल्स के लिए 20 सेकंड लेता है, उसे तेज मॉडल का उपयोग करके या मॉडल कॉल्स को समानांतर में चलाकर तेज किया जा सकता है।

**लागत:** प्रति एजेंट रन खर्च क्या है? AI एजेंट्स LLM कॉल्स पर निर्भर होते हैं जो प्रति टोकन या बाहरी APIs पर बिल किए जाते हैं। बार-बार टूल का उपयोग या कई प्रॉम्प्ट्स लागत को तेजी से बढ़ा सकते हैं। उदाहरण के लिए, यदि एक एजेंट मामूली गुणवत्ता सुधार के लिए LLM को पांच बार कॉल करता है, तो आपको मूल्यांकन करना चाहिए कि लागत उचित है या आप कॉल्स की संख्या को कम कर सकते हैं या सस्ता मॉडल उपयोग कर सकते हैं। रीयल-टाइम मॉनिटरिंग अप्रत्याशित स्पाइक्स (जैसे, अत्यधिक API लूप्स के कारण बग्स) की पहचान करने में भी मदद कर सकता है।

**रिक्वेस्ट एरर्स:** एजेंट ने कितने रिक्वेस्ट्स को विफल किया? इसमें API एरर्स या असफल टूल कॉल्स शामिल हो सकते हैं। उत्पादन में इनसे अधिक मजबूत बनाने के लिए, आप फॉलबैक या रिट्राई सेट कर सकते हैं। उदाहरण के लिए, यदि LLM प्रदाता A डाउन है, तो आप बैकअप के रूप में LLM प्रदाता B पर स्विच कर सकते हैं।

**उपयोगकर्ता प्रतिक्रिया:** सीधे उपयोगकर्ता मूल्यांकन मूल्यवान अंतर्दृष्टि प्रदान करते हैं। इसमें स्पष्ट रेटिंग्स (👍थंब्स-अप/👎डाउन, ⭐1-5 स्टार्स) या टेक्स्ट टिप्पणियां शामिल हो सकती हैं। लगातार नकारात्मक प्रतिक्रिया आपको सतर्क कर देनी चाहिए क्योंकि यह संकेत है कि एजेंट अपेक्षित रूप से काम नहीं कर रहा है।

**अप्रत्यक्ष उपयोगकर्ता प्रतिक्रिया:** उपयोगकर्ता व्यवहार अप्रत्यक्ष प्रतिक्रिया प्रदान करते हैं, भले ही स्पष्ट रेटिंग्स न हों। इसमें तत्काल प्रश्न पुन: स्वरूपण, बार-बार क्वेरी या रिट्राई बटन पर क्लिक करना शामिल हो सकता है। उदाहरण के लिए, यदि आप देखते हैं कि उपयोगकर्ता बार-बार एक ही प्रश्न पूछते हैं, तो यह संकेत है कि एजेंट अपेक्षित रूप से काम नहीं कर रहा है।

**सटीकता:** एजेंट कितनी बार सही या वांछनीय आउटपुट उत्पन्न करता है? सटीकता की परिभाषाएं भिन्न होती हैं (जैसे, समस्या-समाधान की सटीकता, जानकारी पुनर्प्राप्ति सटीकता, उपयोगकर्ता संतुष्टि)। पहला कदम यह परिभाषित करना है कि आपके एजेंट के लिए सफलता कैसी दिखती है। आप स्वचालित जांच, मूल्यांकन स्कोर, या कार्य पूर्णता लेबल्स के माध्यम से सटीकता को ट्रैक कर सकते हैं। उदाहरण के लिए, ट्रेसेस को "सफल" या "विफल" के रूप में चिह्नित करना।

**स्वचालित मूल्यांकन मेट्रिक्स:** आप स्वचालित मूल्यांकन भी सेट कर सकते हैं। उदाहरण के लिए, आप एजेंट के आउटपुट को स्कोर करने के लिए LLM का उपयोग कर सकते हैं, जैसे कि यह सहायक, सटीक है या नहीं। कई ओपन सोर्स लाइब्रेरी भी हैं जो एजेंट के विभिन्न पहलुओं को स्कोर करने में मदद करती हैं। उदाहरण के लिए, [RAGAS](https://docs.ragas.io/) RAG एजेंट्स के लिए या [LLM Guard](https://llm-guard.com/) हानिकारक भाषा या प्रॉम्प्ट इंजेक्शन का पता लगाने के लिए।

व्यवहार में, इन मेट्रिक्स का संयोजन AI एजेंट के स्वास्थ्य का सबसे अच्छा कवरेज प्रदान करता है। इस अध्याय के [उदाहरण नोटबुक](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) में, हम दिखाएंगे कि ये मेट्रिक्स वास्तविक उदाहरणों में कैसे दिखते हैं, लेकिन पहले हम सीखेंगे कि एक सामान्य मूल्यांकन वर्कफ़्लो कैसा दिखता है।

## अपने एजेंट को इंस्ट्रूमेंट करें

ट्रेसिंग डेटा एकत्र करने के लिए, आपको अपने कोड को इंस्ट्रूमेंट करना होगा। उद्देश्य एजेंट कोड को इंस्ट्रूमेंट करना है ताकि ट्रेसेस और मेट्रिक्स उत्पन्न हों जिन्हें अवलोकन प्लेटफ़ॉर्म द्वारा कैप्चर, प्रोसेस और विज़ुअलाइज़ किया जा सके।

**ओपनटेलीमेट्री (OTel):** [OpenTelemetry](https://opentelemetry.io/) LLM अवलोकन के लिए एक उद्योग मानक के रूप में उभरा है। यह टेलीमेट्री डेटा उत्पन्न करने, एकत्र करने और निर्यात करने के लिए APIs, SDKs और उपकरणों का एक सेट प्रदान करता है।

कई इंस्ट्रूमेंटेशन लाइब्रेरी हैं जो मौजूदा एजेंट फ्रेमवर्क्स को wrap करती हैं और OpenTelemetry स्पैन्स को अवलोकन उपकरण में निर्यात करना आसान बनाती हैं। नीचे AutoGen एजेंट को [OpenLit इंस्ट्रूमेंटेशन लाइब्रेरी](https://github.com/openlit/openlit) के साथ इंस्ट्रूमेंट करने का एक उदाहरण है:

```python
import openlit

openlit.init(tracer = langfuse._otel_tracer, disable_batch = True)
```

इस अध्याय के [उदाहरण नोटबुक](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) में दिखाया जाएगा कि अपने AutoGen एजेंट को कैसे इंस्ट्रूमेंट करें।

**मैनुअल स्पैन निर्माण:** जबकि इंस्ट्रूमेंटेशन लाइब्रेरी एक अच्छा आधार प्रदान करती हैं, अक्सर ऐसे मामले होते हैं जहां अधिक विस्तृत या कस्टम जानकारी की आवश्यकता होती है। आप कस्टम एप्लिकेशन लॉजिक जोड़ने के लिए मैनुअल रूप से स्पैन्स बना सकते हैं। अधिक महत्वपूर्ण बात, वे स्वचालित या मैनुअल रूप से बनाए गए स्पैन्स को कस्टम विशेषताओं (टैग या मेटाडेटा के रूप में भी जाना जाता है) के साथ समृद्ध कर सकते हैं। इन विशेषताओं में व्यवसाय-विशिष्ट डेटा, मध्यवर्ती गणनाएं, या कोई भी संदर्भ शामिल हो सकता है जो डिबगिंग या विश्लेषण के लिए उपयोगी हो, जैसे `user_id`, `session_id`, या `model_version`।

[Langfuse Python SDK](https://langfuse.com/docs/sdk/python/sdk-v3) के साथ मैनुअल रूप से ट्रेसेस और स्पैन्स बनाने का उदाहरण:

```python
from langfuse import get_client
 
langfuse = get_client()
 
span = langfuse.start_span(name="my-span")
 
span.end()
```

## एजेंट मूल्यांकन

अवलोकन हमें मेट्रिक्स देता है, लेकिन मूल्यांकन वह प्रक्रिया है जिसमें उस डेटा का विश्लेषण किया जाता है (और परीक्षण किए जाते हैं) ताकि यह निर्धारित किया जा सके कि AI एजेंट कितना अच्छा प्रदर्शन कर रहा है और इसे कैसे सुधार किया जा सकता है। दूसरे शब्दों में, एक बार जब आपके पास ट्रेसेस और मेट्रिक्स होते हैं, तो आप उनका उपयोग एजेंट का न्याय करने और निर्णय लेने के लिए कैसे करते हैं?

नियमित मूल्यांकन महत्वपूर्ण है क्योंकि AI एजेंट अक्सर गैर-निर्धारक होते हैं और विकसित हो सकते हैं (अपडेट्स या मॉडल व्यवहार में बदलाव के माध्यम से) – बिना मूल्यांकन के, आप नहीं जान पाएंगे कि आपका "स्मार्ट एजेंट" वास्तव में अपना काम अच्छी तरह कर रहा है या नहीं।

AI एजेंट्स के लिए मूल्यांकन के दो श्रेणियां हैं: **ऑनलाइन मूल्यांकन** और **ऑफलाइन मूल्यांकन**। दोनों मूल्यवान हैं, और वे एक-दूसरे को पूरक करते हैं। हम आमतौर पर ऑफलाइन मूल्यांकन से शुरू करते हैं, क्योंकि यह किसी भी एजेंट को तैनात करने से पहले न्यूनतम आवश्यक कदम है।

### ऑफलाइन मूल्यांकन

![Langfuse में डेटासेट आइटम्स](https://langfuse.com/images/cookbook/example-autogen-evaluation/example-dataset.png)

यह नियंत्रित सेटिंग में एजेंट का मूल्यांकन करना शामिल करता है, आमतौर पर परीक्षण डेटासेट्स का उपयोग करके, न कि लाइव उपयोगकर्ता क्वेरी। आप क्यूरेटेड डेटासेट्स का उपयोग करते हैं जहां आप जानते हैं कि अपेक्षित आउटपुट या सही व्यवहार क्या है, और फिर अपने एजेंट को उन पर चलाते हैं।

उदाहरण के लिए, यदि आपने एक गणित वर्ड-प्रॉब्लम एजेंट बनाया है, तो आपके पास 100 समस्याओं के [परीक्षण डेटासेट](https://huggingface.co/datasets/gsm8k) हो सकते हैं जिनके उत्तर ज्ञात हैं। ऑफलाइन मूल्यांकन अक्सर विकास के दौरान किया जाता है (और CI/CD पाइपलाइनों का हिस्सा हो सकता है) ताकि सुधारों की जांच की जा सके या रिग्रेशन से बचा जा सके। लाभ यह है कि यह **दोहराने योग्य है और आप स्पष्ट सटीकता मेट्रिक्स प्राप्त कर सकते हैं क्योंकि आपके पास ग्राउंड ट्रुथ है**। आप उपयोगकर्ता क्वेरी का अनुकरण कर सकते हैं और एजेंट की प्रतिक्रियाओं को आदर्श उत्तरों के खिलाफ माप सकते हैं या ऊपर वर्णित स्वचालित मेट्रिक्स का उपयोग कर सकते हैं।

ऑफलाइन मूल्यांकन के साथ मुख्य चुनौती यह सुनिश्चित करना है कि आपका परीक्षण डेटासेट व्यापक और प्रासंगिक बना रहे – एजेंट एक निश्चित परीक्षण सेट पर अच्छा प्रदर्शन कर सकता है लेकिन उत्पादन में बहुत अलग क्वेरी का सामना कर सकता है। इसलिए, आपको वास्तविक दुनिया के परिदृश्यों को दर्शाने वाले नए एज केस और उदाहरणों के साथ परीक्षण सेट को अपडेट रखना चाहिए। छोटे "स्मोक टेस्ट" मामलों और बड़े मूल्यांकन सेट्स का मिश्रण उपयोगी है: छोटे सेट्स त्वरित जांच के लिए और बड़े सेट्स व्यापक प्रदर्शन मेट्रिक्स के लिए।

### ऑनलाइन मूल्यांकन 

![अवलोकन मेट्रिक्स का अवलोकन](https://langfuse.com/images/cookbook/example-autogen-evaluation/dashboard.png)

यह लाइव, वास्तविक दुनिया के वातावरण में एजेंट का मूल्यांकन करने को संदर्भित करता है, यानी उत्पादन में वास्तविक उपयोग के दौरान। ऑनलाइन मूल्यांकन में वास्तविक उपयोगकर्ता इंटरैक्शन पर एजेंट के प्रदर्शन की निगरानी करना और परिणामों का लगातार विश्लेषण करना शामिल है।

उदाहरण के लिए, आप लाइव ट्रैफिक पर सफलता दर, उपयोगकर्ता संतुष्टि स्कोर, या अन्य मेट्रिक्स को ट्रैक कर सकते हैं। ऑनलाइन मूल्यांकन का लाभ यह है कि यह **उन चीजों को कैप्चर करता है जिन्हें आप लैब सेटिंग में अनुमान नहीं लगा सकते हैं** – आप समय के साथ मॉडल ड्रिफ्ट का अवलोकन कर सकते हैं (यदि एजेंट की प्रभावशीलता इनपुट पैटर्न बदलने के साथ घटती है) और अप्रत्याशित क्वेरी या स्थितियों को पकड़ सकते हैं जो आपके परीक्षण डेटा में नहीं थे। यह दिखाता है कि एजेंट वास्तविक दुनिया में कैसे व्यवहार करता है।

ऑनलाइन मूल्यांकन में अक्सर अप्रत्यक्ष और प्रत्यक्ष उपयोगकर्ता प्रतिक्रिया एकत्र करना शामिल होता है, जैसा कि चर्चा की गई है, और संभवतः शैडो टेस्ट या A/B टेस्ट चलाना (जहां एजेंट का नया संस्करण पुराने के साथ तुलना करने के लिए समानांतर में चलता है)। चुनौती यह है कि लाइव इंटरैक्शन के लिए विश्वसनीय लेबल्स या स्कोर प्राप्त करना मुश्किल हो सकता है – आप उपयोगकर्ता प्रतिक्रिया या डाउनस्ट्रीम मेट्रिक्स (जैसे उपयोगकर्ता ने परिणाम पर क्लिक किया या नहीं) पर निर्भर हो सकते हैं।

### दोनों का संयोजन

ऑनलाइन और ऑफलाइन मूल्यांकन परस्पर अनन्य नहीं हैं; वे अत्यधिक पूरक हैं। ऑनलाइन मॉनिटरिंग से अंतर्दृष्टि (जैसे, नए प्रकार की उपयोगकर्ता क्वेरी जहां एजेंट खराब प्रदर्शन करता है) का उपयोग ऑफलाइन परीक्षण डेटासेट्स को बढ़ाने और सुधारने के लिए किया जा सकता है। इसके विपरीत, जो एजेंट ऑफलाइन परीक्षणों में अच्छा प्रदर्शन करते हैं, उन्हें अधिक आत्मविश्वास के साथ तैनात किया जा सकता है और ऑनलाइन मॉनिटर किया जा सकता है।

वास्तव में, कई टीमें एक लूप अपनाती हैं:

_ऑफलाइन मूल्यांकन करें -> तैनात करें -> ऑनलाइन मॉनिटर करें -> नए विफलता मामलों को एकत्र करें -> ऑफलाइन डेटासेट में जोड़ें -> एजेंट को परिष्कृत करें -> दोहराएं।_

## सामान्य समस्याएं

जैसे-जैसे आप AI एजेंट्स को उत्पादन में तैनात करते हैं, आप विभिन्न चुनौतियों का सामना कर सकते हैं। यहां कुछ सामान्य समस्याएं और उनके संभावित समाधान दिए गए हैं:

| **समस्या**    | **संभावित समाधान**   |
| ------------- | ------------------ |
| AI एजेंट कार्यों को लगातार नहीं कर रहा है | - AI एजेंट को दिए गए प्रॉम्प्ट को परिष्कृत करें; उद्देश्यों को स्पष्ट करें।<br>- पहचानें कि कार्यों को उप-कार्य में विभाजित करना और उन्हें कई एजेंट्स द्वारा संभालना कहां मदद कर सकता है। |
| AI एजेंट निरंतर लूप्स में फंस रहा है  | - सुनिश्चित

- परिभाषित पैरामीटर, प्रॉम्प्ट्स, और टूल्स के नामों को बेहतर बनाएं।  
| मल्टी-एजेंट सिस्टम लगातार प्रदर्शन नहीं कर रहा है | - प्रत्येक एजेंट को दिए गए प्रॉम्प्ट्स को परिष्कृत करें ताकि वे विशिष्ट और एक-दूसरे से अलग हों।<br>- एक "रूटिंग" या नियंत्रक एजेंट का उपयोग करके एक पदानुक्रमित प्रणाली बनाएं, जो यह तय करे कि कौन सा एजेंट सही है। |

इनमें से कई समस्याओं की पहचान ऑब्ज़र्वेबिलिटी के साथ अधिक प्रभावी ढंग से की जा सकती है। पहले चर्चा किए गए ट्रेसेस और मेट्रिक्स यह सटीक रूप से पता लगाने में मदद करते हैं कि एजेंट वर्कफ़्लो में समस्या कहां हो रही है, जिससे डिबगिंग और ऑप्टिमाइज़ेशन अधिक कुशल हो जाता है।

## लागत प्रबंधन

यहां AI एजेंट्स को प्रोडक्शन में तैनात करने की लागत प्रबंधन के लिए कुछ रणनीतियां दी गई हैं:

**छोटे मॉडल का उपयोग करना:** छोटे भाषा मॉडल (SLMs) कुछ एजेंटिक उपयोग मामलों में अच्छा प्रदर्शन कर सकते हैं और लागत को काफी हद तक कम कर सकते हैं। जैसा कि पहले उल्लेख किया गया था, प्रदर्शन और बड़े मॉडलों की तुलना करने के लिए एक मूल्यांकन प्रणाली बनाना यह समझने का सबसे अच्छा तरीका है कि आपका उपयोग मामला SLM पर कितना अच्छा काम करेगा। सरल कार्यों जैसे इरादे की पहचान या पैरामीटर निकालने के लिए SLM का उपयोग करने पर विचार करें, जबकि जटिल तर्क के लिए बड़े मॉडलों को आरक्षित रखें।

**राउटर मॉडल का उपयोग करना:** एक समान रणनीति विभिन्न आकारों और मॉडलों का उपयोग करना है। आप जटिलता के आधार पर अनुरोधों को सही मॉडल तक रूट करने के लिए LLM/SLM या सर्वरलेस फ़ंक्शन का उपयोग कर सकते हैं। यह लागत को कम करने में मदद करेगा और साथ ही सही कार्यों पर प्रदर्शन सुनिश्चित करेगा। उदाहरण के लिए, सरल क्वेरीज़ को छोटे, तेज़ मॉडलों तक रूट करें, और केवल जटिल तर्क कार्यों के लिए महंगे बड़े मॉडलों का उपयोग करें।

**प्रतिक्रियाओं को कैश करना:** सामान्य अनुरोधों और कार्यों की पहचान करना और उनके उत्तर एजेंटिक सिस्टम से गुजरने से पहले प्रदान करना, समान अनुरोधों की मात्रा को कम करने का एक अच्छा तरीका है। आप यह पहचानने के लिए एक फ्लो भी लागू कर सकते हैं कि कोई अनुरोध आपके कैश किए गए अनुरोधों से कितना मेल खाता है, इसके लिए अधिक बुनियादी AI मॉडल का उपयोग करें। यह रणनीति अक्सर पूछे जाने वाले प्रश्नों या सामान्य वर्कफ़्लो के लिए लागत को काफी हद तक कम कर सकती है।

## चलिए इसे व्यवहार में देखते हैं

[इस सेक्शन के उदाहरण नोटबुक](../../../10-ai-agents-production/code_samples/10_autogen_evaluation.ipynb) में, हम देखेंगे कि कैसे हम अपने एजेंट की निगरानी और मूल्यांकन के लिए ऑब्ज़र्वेबिलिटी टूल्स का उपयोग कर सकते हैं।

## पिछला पाठ

[मेटाकॉग्निशन डिज़ाइन पैटर्न](../09-metacognition/README.md)

## अगला पाठ

[MCP](../11-mcp/README.md)

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में उपलब्ध मूल दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।